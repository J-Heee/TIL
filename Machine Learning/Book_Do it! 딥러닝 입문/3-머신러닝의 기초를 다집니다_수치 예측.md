# Chapter 3. 머신러닝의 기초를 다집니다 - 수치 예측

## 2021.02.03 - 2021.02.11

### 3-1. 선형 회귀(Linear Regression)
- 선형 회귀는 머신러닝 알고리즘 중 하나로 딥러닝의 기초가 됨
- 2차원 평면에 놓인 점을 표현하는 **1차 함수의 기울기(slope)와 절편(intercept)을 찾아줌**
- 선형 회귀로 찾은 1차 함수를 **선형 회귀로 만든 모델**이라고 부름
- 선형 회귀 모델로 **새 값 x에 대하여 y를 예측**할 수 있음
- ex) 선형 회귀 모델을 만들어 문제를 해결하는 과정
  - 미리 준비한 입력(x: 3,4,5)과 타깃(y: 25,32,39)을 가지고 모델(y=7x+4)을 만든 다음 새 입력(6)에 대한 어떤 값(46)을 예상하는 것

<br>

#### 문제 해결을 위한 당뇨병 환자의 데이터 준비하기
- 목표 : 당뇨병 환자의 1년 후 병의 진전된 정도를 예측하는 모델 만들기
- 문제를 해결하기 위해서는 **충분한 양의 입력 데이터와 타깃 데이터가 필요**
- 머신러닝, 딥러닝 패키지에는 인공지능 학습을 위한 데이터 세트(dataset)가 준비되어 있음
- 사이킷런과 케라스도 다양한 데이터 세트 제공

<br>

- 사이킷런의 당뇨병 환자의 **데이터 세트 사용**
1. load_diabetes() 함수로 당뇨병 데이터 준비하기
   ```
   from sklearn.datasets import load_diabetes   # 사이킷런의 datasets 모듈에 있는 load_diabetes() 함수 임포트
   diabetes = load_diabetes()                   # 매개변수 값 없이 호출, diabetes에 당뇨병 데이터가 저장됨
   ```
   - `diabetes` 변수에 저장된 값의 자료형은 Bunch 클래스로, 파이썬 딕셔너리(dictionary)와 유사
  
<br>

2. 입력과 타깃 데이터의 크기 확인하기
   ```
   print(diabetes.data.shape, diabetes.target.shape)

   # 출력
   # (442, 10) (442,)
   ```
   - diabetes의 속성 중 `data` 속성과 `target` 속성에 입력/타깃 데이터가 넘파이 배열로 저장되어 있음
   - 넘파이 배열의 크기는 `shape` 속성에 저장되어 있음
   - `diabetes.data`
     - 442개 행과 10개 열로 구성된 2차원 배열
     - **샘플(sample)** : **행**, 당뇨병 환자에 대한 특성(수치)으로 이루어진 데이터 1세트
     - 샘플의 **특성(feature)** : **열**, 당뇨병 데이터의 여러 특징들 (ex.환자의 혈압, 혈당, 몸무게 등)
     - 입력데이터의 특성은 속성, 독립 변수(independent variable), 설명 변수(explanatory variable) 등으로 부름
   - `diabetes.target`
     - 442개의 요소를 가진 1차원 배열 

<br>

3. 입력 데이터 자세히 보기
   ```
   diabetes.data[0:3]   # 입력 데이터 앞부분의 샘플 3개 출력
   ```
   ```
   # 출력
   array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,
        -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613],   # 첫 번째 샘플, 특성의 값 10개 나열
       [-0.00188202, -0.04464164, -0.05147406, -0.02632783, -0.00844872,
        -0.01916334,  0.07441156, -0.03949338, -0.06832974, -0.09220405],   # 두 번째 샘플
       [ 0.08529891,  0.05068012,  0.04445121, -0.00567061, -0.04559945,
        -0.03419447, -0.03235593, -0.00259226,  0.00286377, -0.02593034]])  # 세 번째 샘플
   ```

<br>

4. 타깃 데이터 자세히 보기
   ```
   diabetes.target[:3]

   # 출력
   # array([151.,  75., 141.])
   ```
   - **타깃 데이터는 10개의 요소로 구성된 샘플 1개에 대응**
- 입력/타깃 데이터의 수치가 무엇을 의미하는지는 알 필요 없음 => 전문가 영역
- 우리는 입력 데이터와 타깃 데이터의 수치만 보고 둘 사이의 규칙(모델)을 찾으면 되는 것!

<br>

#### 당뇨병 환자 데이터 시각화하기
1. 맷플롯립의 scatter() 함수로 산점도 그리기
   - 당뇨병 데이터 세트의 10개의 특성을 모두 표현하려면 3차원 이상의 그래프 필요
   - 3차원 이상의 그래프는 그릴 수 없으므로 1개의 특성만 사용
   ```
   import matplotlib.pyplot as plt
   plt.scatter(diabetes.data[:, 2], diabetes.target)    # 세 번째 특성과 타깃 데이터로 산점도 그림
   plt.xlabel('x')                                      # x축은 diabetes.data의 세 번째 특성
   plt.ylabel('y')                                      # y축은 diabetes.target 
   plt.show()

   # 그래프가 정비례 관계를 나타냄
   ``` 
2. 훈련 데이터 준비하기
   - 매번 diabetes.data를 입력하는 것은 번거로우니 미리 분리하여 저장해두기
   - 이후에는 x와 y에 있는 데이터를 이용해 모델 훈련
   ```
   x = diabetes.data[:, 2]   # 입력 데이터의 세 번째 특성을 변수 x에 저장
   y = diabetes.target       # 타깃 데이터는 변수 y에 저장
   ```

<br>

### 3-2. 경사 하강법(gradient descent)
- 선형 회귀의 목표
  - 입력 데이터(x)와 타깃 데이터(y)를 통해 **기울기(a)와 절편 (b)를 찾는 것**
  - 산점도 그래프를 잘 표현하는 직선의 방정식을 찾는 것
  - 선형 회귀 문제를 풀기 위한 알고리즘 중 하나가 경사 하강법
- **경사 하강법** : 모델이 데이터를 잘 표현할 수 있도록 **기울기(변화율)를 사용하여 모델을 조금씩 조정하는 최적화 알고리즘**

<br>

#### 딥러닝 분야에서의 표기
- 기울기 a를 w(가중치 의미)나 θ(계수를 의미)로 표기
- y는 ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D) (y-hat)으로 표기
- y = ax + b로 알고 있던 모델을 $\widehat{y}$ = wx + b로 이해하기
- 가중치 w와 절편 b : 알고리즘이 찾은 규칙 의미
- ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D) : 우리가 예측한 값(예측값) 의미

<br>

#### 예측값
- 모델에 새로운 입력값을 넣으면 나오는 **출력값**이 모델을 통해 예측한 값
- ex) y=x+4 모델에서 x에 7을 넣으면 나오는 53이라는 값이 예측값

<br>

#### 예측값으로 올바른 모델을 찾는 방법 (훈련 데이터에 잘 맞는 w와 b를 찾는 방법)
1. 무작위로 w와 b를 정한다(무작위로 모델 만들기).
   ```
   w = 1.0
   b = 1.0
   ```
2. x에서 샘플 하나를 선택하여 ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)을 계산한다(무작위로 모델 예측하기).
   ```
   y_hat = x[0] * w + b    # 훈련 데이터의 첫 번째 샘플 x[0]에 대한 예측값 저장
   print(y_hat)

   # 출력
   # 1.0616962065186886
   ```
3. ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)과 선택한 샘플의 진짜 y를 비교한다(예측한 값과 진짜 정답 비교하기, 틀릴 확률 99%).
   ```
   print(y[0])    # 첫 번째 샘플 x[0]에 대응하는 타깃값 y[0] 출력

   # 출력
   # 151.0
   ```
4. ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)이 y와 더 가까워지도록 w, b를 조정한다(모델 조정하기).
   - 4-1. w값 조절해 예측값 바꾸기
   ```
   w_inc = w + 0.1                  # w값을 0.1만큼 증가
   y_hat_inc = x[0] * w_inc + b     # 다시 예측값 구하기
   print(y_hat_inc)

   # 출력
   # 1.0678658271705574             # y_hat보다 조금 증가함
   ```
   - 4-2. w값 조정한 후 예측값 증가 정도 확인하기
   ```
   w_rate = (y_hat_inc - y_hat) / (w_inc - w)   # y_hat이 증가한 양을 w가 증가한 양으로 나누기
   print(w_rate)                                # 증가 정도 출력

   # 출력
   # 0.061696206518688734
   ```
   - 계산한 값 0.0616⋯ : 첫 번째 훈련 데이터 x[0]에 대한 w의 **변화율**
   - **w의 변화율은 결국 훈련 데이터의 첫 번째 샘플인 x[0]과 동일!** (책 p.58 수식 참고)
5. 모든 샘플을 처리할 때까지 다시 2~4 항목을 반복한다.

<br>

#### 변화율로 가중치 업데이트
- **변화율이 양수**일 때 가중치 업데이트 방법
  - y_hat을 증가시키려면 : **w가 증가하면 y_hat도 증가함**
  - 변화율이 양수이므로, 변화율을 w에 더하는 방법으로 증가 가능 
- **변화율이 음수**일 때 가중치 업데이트 방법
  - y_hat을 증가시키려면 : **w가 감소하면 y_hat은 증가함**
  - 변화율이 음수이므로, 변화율을 w에 더하는 방법으로 증가 가능
- 결론 : 두 경우 모두 **가중치 w를 업데이트 하는 방법은 `w + w_rate`으로 동일!**
  ```
  w_new = w + w_rate
  print(w_new)

  # 출력
  # 1.0616962065186888
  ```

<br>

#### 변화율로 절편 업데이트
- 절편 b에 대한 변화율을 구하고, 변화율로 b 업데이트
  ```
  b_inc = b + 0.1                   # b값을 0.1만큼 증가
  y_hat_inc = x[0] * w + b_inc      # 예측값 구하기
  print(y_hat_inc)

  # 출력
  # 1.1616962065186887


  b_rate = (y_hat_inc - y_hat) / (b_inc - b)    # 변화율 계산
  print(b_rate)

  # 출력
  # 1.0
  ```
  - b가 1만큼 증가하면 y_hat도 1만큼 증가
- 따라서 b를 업데이트 하기 위해서는 변화율이 1이므로 단순히 1 더하기
  ```
  b_new = b + 1
  print(b_new)

  # 출력
  # 2.0
  ```

<br>

#### 변화율로 가중치와 절편 업데이트의 문제점
- y_hat이 y에 한참 미치지 못하는 값인 경우, w와 b를 더 큰 폭으로 수정할 수 없음 (앞에서 변화율만큼 수정했지만 특별한 기준을 정하기 어려움)
- y_hat이 y보다 커지면 y_hat을 감소시키지 못함
- => 오차 역전파로 해결 가능

<br>

#### 오차 역전파(backpropagation)
- ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)과 y의 차이를 이용하여 w와 b 업데이트
- 오차가 연이어 전파되는 모습으로 수행
- **y에서 ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)을 뺀 오차의 양을 변화율에 곱하는 방법**으로 w 업데이트
  - ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)이 y보다 많이 작은 경우 w와 b를 많이 바꿀 수 있음
  - ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)이 y를 지나치면 w와 b의 방향도 바꿔줌

<br>

#### 오차 역전파로 가중치와 절편 더 적절하게 업데이트
1. 오차와 변화율을 곱하여 가중치 업데이트하기
   ```
   err = y[0] - y_hat            # x[0]일 때의 오차
   w_new = w + w_rate * err      # w의 변화율에 오차를 곱하여 업데이트
   b_new = b + 1 * err           # b의 변화율에 오차를 곱하여 업데이트
   print(w_new, b_new)

   # 출력
   # 10.250324555904514 150.9383037934813       w와 b가 각각 큰 폭으로 바뀌었음
   ```
2. 두 번째 샘플 x[1]을 사용하여 오차를 구하고 새로운 w와 b 구하기
   ```
   y_hat = x[1] * w_new + b_new
   err = y[1] - y_hat
   w_rate = x[1]                    # w_rate는 샘플값과 동일
   w_new = w_new + w_rate * err
   b_new = b_new + 1 * err
   print(w_new, b_new)

   # 출력
   # 14.132317616381767 75.52764127612664
   ```
   - w는 4 증가하고, b는 절반으로 감소함
   - 같은 방식으로 모든 샘플을 사용해 가중치와 절편 업데이트
3. 전체 샘플을 반복하기
   ```
   for x_i, y_i in zip(x, y):    # 입력 x배열과 타깃 y배열에서 요소를 하나씩 꺼냄
      y_hat = x_i * w + b
      err = y_i - y_hat          # 오차 계산
      w_rate = x_i
      w = w + w_rate * err       # w 업데이트
      b = b + 1 * err            # b 업데이트
   print(w, b)

   # 출력
   # 587.8654539985689 99.40935564531424
   ```
   - 파이썬 zip() 함수 : 여러 개의 배열에서 동시에 요소를 하나씩 꺼내줌
4. 과정 3을 통해 얻어낸 모델이 전체 데이터 세트를 잘 표현하는지 그래프 그려보기
   - 산점도 위에 w와 b를 사용한 직선 그려보기
   - 직선 그래프 : 시작점과 종료점의 x, y좌표를 plot() 함수에 전달
   ```
   plt.scatter(x, y)                               # 산점도 그리기
   pt1 = (-0.1, -0.1 * w + b)
   pt2 = (0.15, 0.15 * w + b)
   plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])    # x좌표 2개 [-0.1, 0.15] 지정
   plt.xlabel('x')
   plt.ylabel('y')
   plt.show()
   ```
5. 여러 에포크를 반복하기
   - **에포크(epoch)** : 전체 훈련 데이터를 모두 이용하여 한 단위의 작업을 진행하는 것
   - 경사 하강법에서는 주어진 훈련 데이터로 학습을 여러 번 반복함
   ```
   for i in range(1, 100):          # 100번의 에포크 반복
      for x_i, y_i in zip(x, y):    # 입력 x배열과 타깃 y배열에서 요소를 하나씩 꺼냄
         y_hat = x_i * w + b
         err = y_i - y_hat          # 오차 계산
         w_rate = x_i
         w = w + w_rate * err       # w 업데이트
         b = b + 1 * err            # b 업데이트
   print(w, b)
   
   # 출력
   # 913.5973364345905 123.39414383177204
   ```
   - 100번의 에포크를 반복하여 찾은 w는 약 913.6, b는 약 123.4 정도
   - 업데이트된 w와 b를 이용하여 이전처럼 산점도 위에 직선 그래프 그려보기 (위의 코드 참고)
   - 이전보다 직선이 전체 데이터의 경향을 잘 따라감!
   - 이 **데이터에 잘 맞는 머신러닝 모델** 찾기 완료 
     - ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D) = 913.6x + 123.4
6. 모델로 예측하기
   - 입력 x에 없었던 **새로운 데이터에 대한 예측값** 얻기
   ```
   x_new = 0.18
   y_pred = x_new * w + b           # 모델에 x를 넣고 계산
   print(y_pred)                    # 예측값 출력

   # 출력
   # 287.8416643899983
   ```
   ```
   # 해당 데이터를 산점도 위에 표시하여 잘 예측했는지 확인
   plt.scatter(x, y)
   plt.scatter(x_new, y_pred)
   plt.xlabel('x')
   plt.ylabel('y')
   plt.show()
   ```

<br>

#### 지금까지 모델을 만든 과정을 정리하면
1. w와 b를 임의의 값(1.0, 1.0)으로 초기화하고, 훈련 데이터의 샘플을 하나씩 대입하여 y와 ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)의 오차를 구함
2. 1번에서 구한 오차를 w와 b의 변화율에 곱하고, 이 값을 이용하여 w와 b를 업데이트
3. 만약 ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)이 y보다 커지면 오차는 음수가 되어 자동으로 w와 b가 줄어드는 방향으로 업데이트
4. 반대로 ![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D)이 y보다 작으면 오차는 양수가 되고 w와 b는 더 커지도록 업데이트

<br>

