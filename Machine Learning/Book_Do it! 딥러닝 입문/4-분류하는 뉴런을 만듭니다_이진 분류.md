# Chapter 4. 분류하는 뉴런을 만듭니다 - 이진 분류

## 2021.02.15 - 2021.02.17

### 4-1. 초기 인공지능 알고리즘과 로지스틱 회귀
- **이진 분류(binary classification)** : 임의의 샘플 데이터를 True나 False로 구분하는 문제

<br>

#### 여러 개의 특성 사용
- 특성이 2개인 경우의 선형 함수
  - ![수식](https://latex.codecogs.com/svg.latex?z%20%3D%20w_1x_1%20&plus;%20w_2x_2%20&plus;%20b)
- 특성이 n개인 선형 함수
  - ![수식](https://latex.codecogs.com/svg.latex?z%20%3D%20w_1x_1%20&plus;%20w_2x_2%20&plus;%20%5Ccdots%20&plus;%20w_nx_n%20&plus;%20b)
  - ![수식](https://latex.codecogs.com/svg.latex?z%20%3D%20b%20&plus;%20%5Csum_%7Bi%3D1%7D%5En%20w_ix_i)
  - (n번째 특성의 가중치와 입력 의미)

<br>

#### 퍼셉트론(Perceptron)
- 1957년 발표한, 이진 분류 문제에서 최적의 가중치를 학습하는 알고리즘
- **퍼셉트론은 선형 함수에 계단 함수를 추가한 것**
- **계단 함수(step function)** : **샘플을 이진 분류**하기 위해 사용
  - 선형 함수를 통과한 값 z가
  - 0보다 크거나 같으면 : 1 (양성 클래스, positive class)
  - 0보다 작으면 : -1 (음성 클래스, negative class)
- 선형 함수의 결과값에 계단 함수를 적용하여 2개의 클래스 중 하나로 분류하는 알고리즘
- **계단 함수를 통과한 결과값을 사용하여 모델의 가중치와 절편 업데이트**
- 사이킷런 패키지에서 Perceptron이라는 이름으로 클래스 제공

<br>

#### 아달린(Adaline)
- 1960년 발표한, 퍼셉트론을 개선한 **적응형 선형 뉴런(Adaptive Linear Neuron)**
- **아달린은 선형 함수의 결과값을 학습에 사용**
- 계단 함수의 결과는 예측에만 활용
- 즉 역방향 계산이 계단 함수 출력 이후가 아닌, 선형 함수 출력 이후에 진행

<br>

#### 로지스틱 회귀(logistic regression)
- 아달린에서 조금 더 발전한 형태
- **선형 함수**를 통과시켜 얻은 값(z)으로 **-> 활성화 함수**를 통과시켜서 값(a)을 얻고 **-> 임계 함수**로 보내서 예측 수행(![수식](https://latex.codecogs.com/svg.latex?%5Cwidehat%7By%7D))
- **활성화 함수(activation function)** 
  - z를 임계 함수에 보내기 전에 변형시킴
  - 활성화 함수는 의무적으로 **비선형 함수를 사용**
    - 신경망의 각 층에서 수행되는 계산이 또 다른 큰 선형 함수가 되는 문제를 해결하기 위해서
  - 로지스틱 회귀에는 **시그모이드 함수** 사용
    - 선형 함수의 결과를 확률로 바꾸기 위해 사용
- **임계 함수(threshold function)** 
  - 아달린이나 퍼셉트론의 **계단 함수와 비슷한 역할**, 활성화 함수의 출력값을 사용한다는 차이점

<br>
<br>

### 4-2. 시그모이드 함수로 확률 만들기

#### 시그모이드 함수의 역할
- 로지스틱 회귀에서 사용하는 활성화 함수
- **선형 함수의 출력값(z)을 0~1 사이의 확률값으로 변환**시켜주는 역할
- ex) 시그모이드 함수를 통과한 값(a)을 암 종양 판정에 사용하면 '양성 샘플일 확률(악성 종양일 확률)'로 해석 가능
  - 보통 a가 0.5(50%)보다 크면 양성 클래스로 구분

<br>

#### 시그모이드 함수가 만들어지는 과정
- **오즈 비 > 로짓 함수 > 시그모이드 함수**
- (책 p.83-85 수식과 그래프 참고)
- **오즈 비(odds ratio)**
  - **성공 확률과 실패 확률의 비율을 나타내는 통계**
  - 오즈 비를 그래프로 나타내면, p가 0부터 1까지 증가할 때, 오즈 비의 값은 처음에는 천천히 증가하지만, p가 1에 가까워지면 급격히 증가함
- **로짓 함수(logit function)**
  - **오즈 비에 로그 함수를 취하여 만든 함수**
  - p가 0.5일 때 0이 되고, p가 0과 1일 때 각각 무한대로 음수와 양수가 되는 특징 
- **로지스틱 함수**
  - **로짓 함수의 수식을** 가로 축을 z로 놓기 위해서 **z에 대하여 정리한 것**
  - 로지스틱 함수의 그래프는 로짓 함수의 가로와 세로 축을 반대로 뒤집어 놓은 모양
  - 그래프가 S자 형태를 띄어서 **시그모이드 함수(sigmoid function)라고도 부름**

<br>

#### 로지스틱 회귀 중간 정리
- 로지스틱 회귀의 목표는 이진 분류!
- 따라서 -∞ 부터 ∞의 범위를 가지는 선형 함수의 출력값(z)을 조절할 방법 필요
- 시그모이드 함수를 활성화 함수로 사용하여, z를 확률처럼 해석
- 시그모이드 함수의 확률(a)를 0과 1로 구분하기 위해 마지막에 임계 함수 사용
- 결과적으로 입력 데이터(x)를 0 또는 1의 값으로 이진 분류

<br>
<br>

### 4-3. 로지스틱 손실 함수를 경사 하강법에 적용
- 선형 회귀의 목표 : 정답과 예상값의 오차 제곱이 최소가 되는 가중치와 절편을 찾는 것
- 로지스틱 회귀와 같은 **분류의 목표 : 올바르게 분류된 샘플 데이터의 비율 자체를 높이는 것**
  - ex) 사과/배/감을 분류하는 문제에서 사과/배/감으로 분류한 과일 중 진짜 사과/배/감으로 분류한 비율을 높이는 것
- 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니기 때문에 경사 하강법의 손실 함수로 사용 불가능
- 대신 비슷한 목표를 달성할 수 있는 다른 함수를 사용 => 로지스틱 손실 함수

<br>

#### 로지스틱 손실 함수
- 다중 분류를 위한 손실 함수인 **크로스 엔트로피(cross entropy) 손실 함수를 이진 분류 버전으로** 만든 것
- ![로지스틱 손실 함수](https://latex.codecogs.com/svg.latex?L%20%3D%20-%28ylog%28a%29%20&plus;%20%281-y%29log%281-a%29%29)
  - a = 활성화 함수가 출력한 값, y = 타깃
- 이진 분류이므로 타깃의 값은 1 또는 0
  - y가 1인 경우(양성 클래스) : ![](https://latex.codecogs.com/svg.latex?-log%28a%29)
  - y가 0인 경우(음성 클래스) : ![](https://latex.codecogs.com/svg.latex?-log%281-a%29)
- 위의 두 식을 최소로 만들다 보면, a의 값이 우리가 원하는 목표치가 됨
  - 양성 클래스 : 최소로 만들려면 a는 1에 가까워짐
  - 음성 클래스 : 최소로 만들려면 a가 0에 가까워짐
  - 이 값을 계단 함수에 통과시키면 올바르게 분류 작업 수행
- **로지스틱 손실 함수를 최소화하면, a의 값이 우리가 가장 이상적으로 생각하는 값이 된다!**

<br>

#### 로지스틱 손실 함수 미분
- 로지스틱 손실 함수의 **최솟값을 만드는 가중치와 절편을 찾기 위해 로지스틱 손실 함수 미분**
  | | 제곱 오차의 미분 | 로지스틱 손실 함수의 미분 |  
  | ---          | ---          | ---        |
  | 가중치에 대한 미분 | ![수식](https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20SE%7D%7B%5Cpartial%20w%7D%20%3D%20-%28y-%5Chat%7By%7D%29x) | ![수식](https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20w_i%7DL%20%3D%20-%28y-a%29x_i) |
  | 절편에 대한 미분 | ![수식](https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20SE%7D%7B%5Cpartial%20b%7D%20%3D%20-%28y-%5Chat%7By%7D%291)  | ![수식](https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20b%7DL%20%3D%20-%28y-a%291) |
- ![y_hat](https://latex.codecogs.com/svg.latex?%5Chat%7By%7D)이 a로 바뀌었을 뿐 **제곱 오차를 미분한 결과와 동일**
- 로지스틱 회귀의 구현이 03장에서 만든 Neuron 클래스와 비슷할 것 예상 가능
- 로지스틱 손실 함수를 가중치에 대하여 미분할 때 연쇄 법칙 사용 (미분 유도 과정은 책 p.88-91 참고)

<br>

#### 가중치 업데이트 방법
- 가중치에서 로지스틱 손실 함수를 가중치에 대해 미분한 식을 빼기
  - ![수식](https://latex.codecogs.com/svg.latex?w_i%20%3D%20w_i-%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_i%7D%20%3D%20w_i%20&plus;%20%28y-a%29x_i)

#### 절편 업데이트 방법
- 절편에서 로지스틱 손실 할수를 절편에 대해 미분한 식을 빼기
  - ![수식](https://latex.codecogs.com/svg.latex?b%20%3D%20b-%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20b%7D%20%3D%20b%20&plus;%20%28y-a%291)

<br>
<br>

### 4-4. 분류용 데이터 세트 준비
- 사이킷런에 포함된 '위스콘신 유방암 데이터 세트' 사용

#### 유방암 데이터 세트 소개
- 유방암 세포의 특징 10개에 대하여 평균, 표준 오차, 최대 이상치가 기록되어 있음
- **해결할 문제 : 유방암 데이터 샘플이 악성 종양(True)인지** 정상 종양(False)인지 **구분하는 이진 분류 문제**
  |   |의학  |이진 분류|
  |---|---|---  |
  |좋음|양성 종양(정상 종양)|음성 샘플|
  |나쁨|악성 종양|양성 샘플 **<- 해결 과제!**|

<br>

#### 유방암 데이터 세트 준비
1. `load_breast_cancer()` 함수 호출하기
   ```
   from sklearn.datasets import load_breast_cancer
   cancer = load_breast_cancer()                     # Bunch 클래스의 객체 가져옴
   ```
2. 입력 데이터 확인하기
   ```
   print(cancer.data.shape, cancer.target.shape)

   # 출력
   # (569, 30) (569,)
   ```
   - cancer에는 569개의 샘플과 30개의 특성 존재
   ```
   cancer.data[:3]            # 처음 3개의 샘플 출력
   ```
   - 출력된 특성 데이터를 살펴보면 실수 범위의 값이고 양수로 이루어져 있음
   - 30개의 특성을 산점도로 표현하기 어려워서 **박스 플롯(box plot)을 이용하여 각 특성의 사분위(quartile)값을 나타낼 것**
3. 박스 플롯으로 특성의 사분위 관찰하기
   - 박스 플롯은 
     - 1사분위와 3사분위의 값으로 상자를 그리고
     - 그 안에 2사분위(중간값)값을 표시
     - 그 후, 1사분위와 3사분위 사이 거리(interquartile range)의 1.5배만큼 위아래 거리에서 각각 가장 큰 값과 가장 작은 값까지 수염을 그림
     - (책 p.94 참고)
   - 박스 플롯은 상자 그래프, 상자 수염 그래프(box-and-whisker plot)라고도 부름 
   ```
   import matplotlib.pyplot as plt
   import numpy as np

   plt.boxplot(cancer.data)
   plt.xlabel('feature')
   plt.ylabel('value')
   plt.show()
   ```
4. 눈에 띄는 특성 살펴보기
   - 박스 플롯을 보면 4, 14, 24번째 특성이 다른 특성보다 값의 분포가 훨씬 큼
   - 4, 14, 24번째 특성의 인덱스를 리스트로 묶어 전달하면, 각 인덱스의 특성 확인 가능
   ```
   cancer.feature_names[[3,13,23]]          # 인덱스의 특성 확인

   # 출력
   # array(['mean area', 'area error', 'worst area'], dtype='<U23')
   # => 모두 넓이와 관련된 특성
   ```
5. 타깃 데이터 확인하기
   - `cancer.target` 배열 안에는 0(음성 클래스)과 1(양성 클래스)만 들어 있음
   - `넘파이의 unique() 함수` : 고유한 값을 찾아 반환
   - `return_counts 매개변수를 True`로 지정하면 : 고유한 값이 등장하는 횟수까지 세어 반환
   ```
   np.unique(cancer.target, return_counts=True)

   # 출력
   # (array([0, 1]), array([212, 357]))
   ```
   - 출력된 왼쪽의 값 : cancer.target에 들어 있는 고유한 값 0, 1 의미
   - 출력된 오른쪽 값 : 타깃 데이터에는 212개의 음성 클래스(정상 종양), 357개의 양성 클래스(악성 종양)이 있음을 의미
6. 훈련 데이터 세트 저장하기
   ```
   # 예제 데이터 세트를 x,y변수에 저장
   x = cancer.data
   y = cancer.target
   ```

<br>
<br>

### 4-5. 로지스틱 회귀를 위한 뉴런 만들기
- 훈련된 모델이 실전에서 얼마나 좋은 성능을 내는지 제대로 평가할 수 있는 방법이 필요함

<br>

#### 모델의 성능 평가를 위한 훈련 세트와 테스트 세트
- **일반화 성능(generalization performance) : 훈련된 모델의 실전 성능**
- '과도하게 낙관적으로 일반화 성능을 추정한다'
  - : 모델을 학습시킨 훈련 데이터 세트로 다시 모델의 성능을 평가하면, 그 모델은 당연히 좋은 성능이 나올 것
- 올바르게 모델의 성능을 측정하려면, 훈련 데이터 세트를 두 덩어리로 나누어
  - **훈련 세트(training set)** : 훈련에 사용 
  - **테스트 세트(test set)** : 테스트에 사용
- **훈련 데이터 세트를 훈련 세트와 테스트 세트로 나누는 규칙**
  - 훈련 데이터 세트를 나눌 때는, 테스트 세트보다 **훈련 세트가 더 많아야 함**
  - 훈련 데이터 세트를 나누기 전에, **양성/음성 클래스가** 훈련 세트나 테스트 세트의 **어느 한쪽에 몰리지 않도록 골고루 섞어야 함**
- 사이킷런에 준비되어 있는 편리한 도구 사용 가능

<br>

#### 훈련 세트와 테스트 세트로 나누기
- 양성 클래스와 음성 클래스의 비율을 일정하게 유지하도록 나눠야 함
1. **`train_test_split()` 함수로 훈련 데이터 세트 나누기**
   ```
   from sklearn.model_selection import train_test_split
   x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)
   ```
   - `train_test_split()` : 기본적으로 입력된 훈련 데이터 세트를 **훈련 세트 75%, 테스트 세트 25% 비율로** 나눠줌
   - 입력 데이터 x, 타깃 데이터 y
   - `stratify=y`
     - stratify는 훈련 데이터를 나룰 때 클래스 비율을 동일하게 함
     - 함수가 기본적으로 데이터를 나누기 전에 섞지만, 일부 클래스 비율이 불균형한 경우에는 stratify를 y로 지정해야 함 
   - `test_size=0.2`
     - 이 매개변수에 테스트 세트의 비율을 전달하면 비율 조정 가능
     - 여기서는 입력된 데이터 세트의 20%를 테스트 세트로, 80%를 훈련 세트로 나누기 위해 test_size에 0.2 전달 
   - `random_state=42`
     - 함수가 무작위로 데이터 세트를 섞은 다음 나눔
     - 섞은 다음 나눈 결과가 항상 일정하도록 이 매개변수에 난수 초기값 42 설정
     - (책이므로 실험 결과를 똑같이 재현하기 위해 사용, 실전에서는 사용 불필요) 
2. 나누어진 결과 확인하기
   ```
   print(x_train.shape, x_test.shape)

   # 출력
   # (455, 30) (114, 30)
   ```
   - 4:1의 비율로 잘 나누어짐
   - 30은 유방암 데이터 세트의 특성 개수 의미
3. unique() 함수로 훈련 세트의 타깃 확인하기
   ```
   np.unique(y_train, return_counts=True)       # 훈련 세트의 타깃 안에 있는 클래스의 개수 확인

   # 출력
   # (array([0, 1]), array([170, 285]))
   ```
   - 전체 훈련 데이터 세트의 클래스 비율과 거의 비슷한 구성 (클래스 비율 유지)

<br>

#### 로지스틱 회귀 구현
- 정방향으로 데이터가 흘러가는 과정(정방향 계산)과 가중치를 업데이트하기 위해 역방향으로 데이터가 흘러가는 과정(역방향 계산)을 구현해야 함
- ```
  class LogisticNeuron:
    
    def __init__(self):
        self.w = None
        self.b = None

    def forpass(self, x):
        z = np.sum(x * self.w) + self.b    # 직선 방정식 계산
        return z

    def backprop(self, x, err):
        w_grad = x * err                   # 가중치에 대한 그래디언트 계산
        b_grad = 1 * err                   # 절편에 대한 그래디언트 계산
        return w_grad, b_grad
  ```
  - `__init()__`
    - 입력 데이터의 특성이 많아서 가중치와 절편을 미리 초기화하지 않음
    - 나중에 입력 데이터를 보고 특성 개수에 맞게 가중치 결정
  - `forpass()`
    - 가중치와 입력 특성의 곱을 모두 더하기 위해 넘파이 함수 `np.sum()` 사용함
    - 넘파이 배열에 사칙연산을 적용하면, 자동으로 배열의 요소끼리 계산
    - 넘파이 배열을 `np.sum()` 함수의 인자로 전달하면, 각 요소를 모두 더한 값을 반환
- **로지스틱 뉴런 구현 완료!**

<br>

#### 훈련하는 메서드 구현
- 기본 구조는 03장의 Neuron 클래스와 같지만, 활성화 함수(activation()) 추가
1. `fit()` 메서드 구현하기
   ```
   def fit(self, x, y, epochs=100):
      self.w = np.ones(x.shape[1])                      # 가중치 초기화
      self.b = 0                                        # 절편 초기화
      for i in range(epochs):                           # epochs만큼 반복
          for x_i, y_i in zip(x, y):                    # 모든 샘플에 대해 반복
              z = self.forpass(x_i)                     # 정방향 계산
              a = self.activation(z)                    # 활성화 함수 적용
              err = -(y_i - a)                          # 오차 계산
              w_grad, b_grad = self.backprop(x_i, err)  # 역방향 계산
              self.w -= w_grad                          # 가중치 업데이트
              self.b -= b_grad                          # 절편 업데이트
   ```
   - 역방향 계산에는 로지스틱 손실 함수의 도함수를 적용
   - `np.ones()` : 입력된 매개변수와 동일한 크기의 배열을 만들고, 값을 모두 1로 채움
2. `activation()` 메서드 구현하기
   ```
   def activation(self, z):
      z = np.clip(z, -100, None)                 # 안전한 np.exp() 계산을 위해
      a = 1 / (1 + np.exp(-z))                   # 시그모이드 계산
   ```
   - `np.exp()` : 자연 상수의 지수 함수를 계산하는 함수, 이 함수를 사용하여 시그모이드 함수 만듦

<br>

#### 예측하는 메서드 구현
1. `predict()` 메서드 구현하기
   ```
   def predict(self, x):
      z = [self.forpass(x_i) for x_i in x]      # 정방향 계산
      a = self.activation(np.array(z))          # 활성화 함수 적용
      return a > 0.5
   ```
   - 매개변수 값으로 입력값 x가 2차원 배열로 전달된다고 가정하고 구현
   - **예측값은 입력값을 선형 함수, 활성화 함수, 임계 함수 순서로 통과시키면 구할 수 있음**
   - z의 계산으로 파이썬의 리스트 내포 문법 사용
     - [] 안에 for문을 삽입하여 새 리스트를 만드는 간결한 문법
     - x의 행을 하나씩 꺼내어 forpass()에 적용하고 그 결과를 새 리스트(z)로 만드는 것
     - z는 넘파이 배열로 바꿔 activation()에 전달

<br>

#### 로지스틱 회귀 모델 훈련
1. 모델 훈련하기
   ```
   neuron = LogisticNeuron()          # LogisticNeuron 클래스의 객체 생성
   neuron.fit(x_train, y_train)       # 훈련 세트와 함께 fit() 호출
   ```
2. 테스트 세트 사용해 모델의 정확도 평가하기
   ```
   np.mean(neuron.predict(x_test) == y_test)

   # 출력
   # 0.8245614035087719
   ```
   - 모델에 테스트 세트를 사용해 예측값을 넣고 예측한 값이 맞는지 비교
   - `np.mean()` : 매개변수 값으로 전달한 **비교문 결과(넘파이 배열)의 평균 계산**
   - 계산 결과는 **올바르게 예측한 샘플의 비율 : 정확도(accuracy)**
   - **82%의 정확도**를 가진 로지스틱 회귀 구현!

<br>
<br>

### 4-6. 로지스틱 회귀 뉴런으로 단일층 신경망 만들기
- 로지스틱 회귀는 단일층 신경망과 동일하지만, 신경망 관련 개념 알아보기

#### 일반적인 신경망의 모습 (책 p.105 참고)
- 일반적인 신경망의 **가장 왼쪽을 입력층(input layer)**, **가장 오른쪽을 출력층(output layer)**, **가운데 층들을 은닉층(hidden layer)** 라고 부름
- **단일층 신경망** : 은닉층이 없이, **입력층과 출력층만 가지는 신경망**
  - **로지스틱 회귀가 해당함**

<br>

#### 단일층 신경망 구현
- 위에서 구현한 LogisticNeuron 클래스는 이미 단일층 신경망의 역할을 할 수 있음
- 그러나 몇 가지 유용한 기능의 추가를 위해 단일층 신경망 다시 구현

<br>

#### 추가 기능1 - 손실 함수의 결과값 저장 기능 추가
  - 선형 회귀나 로지스틱 회귀는 모두 경사 하강법 사용
  - 경사 하강법은 손실 함수(제곱 오차 손실 함수, 로지스틱 손실 함수)의 결과값을 최소화하는 방향으로 가중치 업데이트
  - 만약, 손실 함수의 결과값이 줄어들지 않는다면 뭔가 잘못된 것이므로, 그 값의 관찰 필요

<br>

#### 여러 가지 경사 하강법 알아보기
- **확률적 경사 하강법**(stochastic gradient descent) : **지금까지 사용한 경사 하강법, 샘플 데이터 1개에 대한 그레이디언트 계산**
  - 1개마다 그레이디언트를 계산하여 가중치를 업데이트하므로 계산 비용은 적음
  - 가중치가 최적값에 수렴하는 과정이 불안정함
- **배치 경사 하강법**(batch gradient descent) : **전체 훈련 세트를 사용하여 한 번에 그레이디언트를 계산하는 방식**
  - 가중치가 최적값에 수렴하는 과정은 안정적
  - 그만큼 계산 비용이 많이 필요
- **미니 배치 경사 하강법**(mini-batch gradient descent) : **배치 크기를 작게 하여(훈련 세트를 여러 번 나누어) 처리하는 방식**
  - **위 두가지 방식의 장점을 절충한 것**
  - 확률적 경사 하강법보다는 매끄럽고, 배치 경사 하강법보다는 덜 매끄러운 그래프가 그려짐 (책 p.108 그래프 참고)

<br>

#### 추가 기능2 - 매 에포크마다 훈련 세트의 샘플 순서를 섞어 사용하기
- 모든 경사 하강법들은 **매 에포크마다 훈련 세트의 샘플 순서를 섞어 가중치의 최적값을 계산**해야 함
  - 가중치 최적값의 탐색 과정이 다양해져 최적값을 제대로 찾을 수 있음
- **넘파이 배열의 인덱스를 섞은 후, 인덱스 순서대로 샘플을 뽑는 방법**
  - 훈련 세트의 샘플 순서를 섞는 전형적인 방법
  - 훈련 세트 자체를 섞는 것보다 효율적이고 빠름
  - `np.random.permutation()` 사용하면 구현 가능

<br>

#### 추가 기능3 - 정확도를 계산하는 score() 메서드 추가
- 정확도를 직접 계산할 때 사용했던 `np.mean()` 함수 사용

<br>

#### 기능을 모두 추가한 단일층 신경말 클래스 완성
```
class SingleLayer:
    
    def __init__(self):
        self.w = None
        self.b = None
        self.losses = []        # 손실 함수의 결과값 저장할 리스트

    def forpass(self, x):
        z = np.sum(x * self.w) + self.b  # 직선 방정식 계산
        return z

    def backprop(self, x, err):
        w_grad = x * err    # 가중치에 대한 그래디언트 계산
        b_grad = 1 * err    # 절편에 대한 그래디언트 계산
        return w_grad, b_grad

    def activation(self, z):
        z = np.clip(z, -100, None) # 안전한 np.exp() 계산을 위해
        a = 1 / (1 + np.exp(-z))  # 시그모이드 계산
        return a
        
    def fit(self, x, y, epochs=100):
        self.w = np.ones(x.shape[1])               # 가중치 초기화
        self.b = 0                                 # 절편 초기화
        for i in range(epochs):                    # epochs만큼 반복
            loss = 0

            # 인덱스를 섞기
            indexes = np.random.permutation(np.arange(len(x)))
            for i in indexes:                      # 모든 샘플에 대해 반복
                z = self.forpass(x[i])             # 정방향 계산
                a = self.activation(z)             # 활성화 함수 적용
                err = -(y[i] - a)                  # 오차 계산
                w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산
                self.w -= w_grad                   # 가중치 업데이트
                self.b -= b_grad                   # 절편 업데이트
                # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적
                a = np.clip(a, 1e-10, 1-1e-10)
                loss += -(y[i]*np.log(a)+(1-y[i])*np.log(1-a))
            # 에포크마다 평균 손실을 저장
            self.losses.append(loss/len(y))
    
    def predict(self, x):
        z = [self.forpass(x_i) for x_i in x]     # 정방향 계산
        return np.array(z) > 0                   # 스텝 함수 적용
    
    def score(self, x, y):
        return np.mean(self.predict(x) == y)
```

<br>

#### 단일층 신경망 훈련
1. 단일층 신경망 훈련하고 정확도 출력하기
   ```
   layer = SingleLayer()              # 객체 생성
   layer.fit(x_train, y_train)        # 훈련 세트로 신경망 훈련
   layer.score(x_test, y_test)        # 정확도 출력

   # 출력
   # 0.8859649122807017
   ```
   - LogisticNeuron과 마찬가지로 에포크 매개변수의 기본값 100을 그대로 사용했는데도 **정확도가 훨씬 좋아짐**
   - => **에포크마다 훈련 세트를 무작위로 섞어 손실 함수의 값을 줄였기 때문!**
2. 손실 함수 누적값 확인하기
   ```
   plt.plot(layer.losses)
   plt.xlabel('epoch')
   plt.ylabel('loss')
   plt.show()
   ```
   - 정말 줄어들었는지 손실 함수의 결과값을 그래프로 그려 확인
- 성공적으로 가장 기초적인 신경망 알고리즘 구현!
- **신경망 알고리즘은 로지스틱 회귀 알고리즘을 확장한 네트워크로 생각 가능**
- 지금까지 선형 회귀, 로지스틱 회귀 등 신경망 알고리즘들을 직접 구현함
- 하지만, **사이킷런에는 이런 알고리즘들이 미리 구현**되어 있음!

<br>
<br>

### 4-7. 사이킷런으로 로지스틱 회귀 수행
- `SGDClassifier` : 사이킷런의 경사 하강법이 구현된 클래스
- 이 클래스는 여러 가지 문제에 경사 하강법 적용 가능

<br>

#### 사이킷런으로 경사 하강법 적용
1. 로지스틱 손실 함수 지정하기
   ```
   from sklearn.linear_model import SGDClassifier
   sgd = SGDClassifier(loss='log', max_iter=100, tol=1e-3, random_state=42)
   ```
   - `loss` : 클래스에 로지스틱 회귀를 적용하려면, 손실 함수로 log 지정 필요
   - `max_iter` : 반복 횟수를 100으로 지정
   - `tol` : 반복할 때마다 로지스틱 손실 함수의 값이 tol에 지정한 값만큼 감소되지 않으면, 반복을 중단하도록 설정
   - `random_state` : 반복 실행했을 때 결과를 동일하게 재현하기 위해 난수 초기값을 42로 설정
2. 사이킷런으로 훈련하고 평가하기
   ```
   sgd.fit(x_train, y_train)          # 훈련
   sgd.score(x_test, y_test)          # 정확도 계산

   # 출력
   # 0.8333333333333334
   ```
   - 사이킷런의 SGDClassifier 클래스에는 지금까지 우리가 직접 구현한 메서드가 이미 준비되어 있음
     - `fit()`, `score()`
3. 사이킷런으로 예측하기
   ```
   sgd.predict(x_test[0:10])          # 예측, 배열의 슬라이싱 사용하여 테스트 세트에서 10개 샘플만 뽑아 예측

   # 출력
   # array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0])
   ```
   - 사이킷런의 SGDClassifier 클래스에는 예측을 위한 `predict()` 메서드도 구현되어 있음
   - 사이킷런은 입력 데이터로 2차원 배열만 받아들임
     - 샘플 하나를 주입하더라도 2차원 배열로 만들어야 함