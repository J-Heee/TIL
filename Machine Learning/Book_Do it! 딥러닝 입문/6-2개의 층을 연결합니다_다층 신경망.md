# Chapter 6. 2개의 층을 연결합니다 - 다층 신경망

## 2021.02.18

### 6-1. 신경망 알고리즘을 벡터화하여 한 번에 전체 샘플을 사용
- 머신러닝에서는 훈련 데이터를 2차원 배열로 표현하는 경우가 많음
- 2차원 배열은 행을 샘플, 열을 특성으로 생각하면 행렬로 이해 가능

<br>

#### 벡터화된 연산은 알고리즘의 성능을 올린다
- 넘파이, 머신러닝, 딥러닝 패키지들은 **벡터화(vectorization)된 연산을 통해 빠른 연산 속도를 제공**
- 배치 경사 하강법을 `SingleLayer` 클래스에 적용하면 벡터화된 연산 사용 가능
- **배치 경사 하강법으로 성능을 올림**
  - 지금까지 사용한 경사 하강법 알고리즘들(선형 회귀, 로지스틱 회귀)은 '확률적 경사 하강법' 사용
    - 가중치를 1번 업데이트할 때 1개의 샘플을 사용하므로, 손실 함수의 전역 최솟값을 불안정하게 찾음
  - **배치 경사 하강법** : **가중치를 1번 업데이트할 때 전체 샘플을 사용**하므로
    - 손실 함수의 전역 최솟값을 안정적으로 찾음
    - 알고리즘 1번 수행당 계산 비용이 많이 든다는 단점
    - 전체 데이터 세트의 크기가 너무 크면 사용하지 못하는 경우도 있음

<br>

#### 벡터 연산과 행렬 연산
- 벡터화된 연산을 제대로 사용하려면 벡터 연산과 행렬 연산을 알아야 함
- **점 곱(dot product)** 또는 **스칼라 곱(scalar product)**
  - 신경망에서 자주 사용하는 벡터 연산 중 하나
  - **두 벡터를 곱하여 합을 구하는 계산 의미** (np.sum(x * self.w))
    - x와 y는 벡터, 벡터는 볼드로 표기
- **점 곱을 행렬 곱셈으로 표현** (책 p.159 수식 참고)
  - 넘파이의 `np.dot()` 함수 : 행렬의 곱셈 계산
  - np.sum(x * self.w) + self.b 수정
    ```
    z = np.dot(x, self.w) + self.b
    ```
- **전체 샘플에 대한 가중치 곱의 합을 행렬 곱셈으로 구할 수 있음**
  - 훈련 데이터의 샘플은 각 샘플이 하나의 행으로 이루어져 있으므로 행렬 곱셈을 적용하면, 샘플의 특성과 가중치를 곱하여 더한 행렬을 얻을 수 있음
  - 행렬 곱셈을 통해 만들어지는 **결과 행렬의 크기** : 첫 번재 행렬의 행과 두번째 행렬의 열이 됨
  - 첫 번재 행렬의 열과 두 번째 행렬의 행 크기는 반드시 같아야 함
  ```
  np.dot(x, w)
  ```

<br>

#### SingleLayer 클래스에 배치 경사 하강법 적용
1. 넘파이와 맷플롯립 임포트
   ```
   import numpy as np
   import matplotlib.pyplot as plt
   ```
2. 위스콘신 유방암 데이터 세트를 훈련, 검증 테스트 세트로 나누고 데이터 살펴보기
   ```
   from sklearn.datasets import load_breast_cancer
   from sklearn.model_selection import train_test_split
   
   cancer = load_breast_cancer()
   x = cancer.data
   y = cancer.target
   x_train_all, x_test, y_train_all, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)
   x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)
   ```
3. 훈련 세트와 검증 세트의 크기 확인
   ```
   print(x_train.shape, x_val.shape)

   # 출력
   # (364, 30) (91, 30)
   ```
4. 정방향 계산을 행렬 곱셈으로 표현하기
   - 훈련 세트와 가중치를 곱한 다음 절편을 더함
   - (책 p.161 참고)
5. 그레이디언트 계산 이해하기
   - 오차와 입력 데이터의 곱
   - 이후 그레이디언트 평균값을 계산할 때 이 값을 다시 전체 샘플 수로 나눠야 함
   - (책 p.162 참고)
6. `forpass()`, `backprop()` 메서드에 배치 경사 하강법 적용하기
   ```
   def forpass(self, x):
       z = np.dot(x, self.w) + self.b        # 선형 출력 계산
       return z
    
   def backprop(self, x, err):
       m = len(x)
       w_grad = np.dot(x.T, err) / m         # 가중치에 대한 평균 그래디언트 계산
       b_grad = np.sum(err) / m              # 절편에 대한 평균 그래디언트 계산
       return w_grad, b_grad
   ```
   - `len()` 함수 : 넘파이 배열의 행 크기 반환
7. `fit()` 메서드 수정하기
   - 가장 큰 변화가 있는 메서드
   - 원래는 에포크를 위한 for문과 훈련 세트를 순회하기 위한 for문이 있었음
   - 배치 경사 하강법에서는 `forpass()`와 `backprop()`에서 전체 샘플을 한번에 계산하므로 두 번째 for문 삭제
   - 전체 구조는 확률적 경사 하강법과 비슷하지만 한 단계 for문의 삭제로 코드가 훨씬 간단해짐
   - 활성화 출력 a가 열 벡터이므로, 맞춰서 타깃값을 (m,1)크기의 열 벡터로 변환
8. 나머지 메서드 수정하기
   - `predict()` 메서드에서 사용했던 리스트 내포와 `update_val_loss()` 메서드도 더 간단해짐
- 지금까지 배치 경사 하강법을 적용한 `SingleLayer` 클래스의 전체 코드
  ```
  class SingleLayer:
    
    def __init__(self, learning_rate=0.1, l1=0, l2=0):
        self.w = None              # 가중치
        self.b = None              # 절편
        self.losses = []           # 훈련 손실
        self.val_losses = []       # 검증 손실
        self.w_history = []        # 가중치 기록
        self.lr = learning_rate    # 학습률
        self.l1 = l1               # L1 손실 하이퍼파라미터
        self.l2 = l2               # L2 손실 하이퍼파라미터

    def forpass(self, x):
        z = np.dot(x, self.w) + self.b        # 선형 출력 계산
        return z

    def backprop(self, x, err):
        m = len(x)
        w_grad = np.dot(x.T, err) / m         # 가중치에 대한 그래디언트 계산
        b_grad = np.sum(err) / m              # 절편에 대한 그래디언트 계산
        return w_grad, b_grad

    def activation(self, z):
        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해
        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산
        return a
  
          
    def fit(self, x, y, epochs=100, x_val=None, y_val=None):
        y = y.reshape(-1, 1)                  # 타깃을 열 벡터로 바꿈
        y_val = y_val.reshape(-1, 1)          # 검증용 타깃을 열 벡터로 바꿈
        m = len(x)                            # 샘플 개수 저장
        self.w = np.ones((x.shape[1], 1))     # 가중치 초기화
        self.b = 0                            # 절편 초기화
        self.w_history.append(self.w.copy())  # 가중치 기록

        # epochs만큼 반복
        for i in range(epochs):
            z = self.forpass(x)               # 정방향 계산 수행
            a = self.activation(z)            # 활성화 함수 적용
            err = -(y - a)                    # 오차 계산
            # 오차를 역전파하여 그래디언트 계산
            w_grad, b_grad = self.backprop(x, err)
            # 그래디언트에 페널티 항의 미분 값을 더함
            w_grad += (self.l1 * np.sign(self.w) + self.l2 * self.w) / m
            # 가중치와 절편 업데이트
            self.w -= self.lr * w_grad
            self.b -= self.lr * b_grad
            # 가중치 기록
            self.w_history.append(self.w.copy())
            # 안전한 로그 계산을 위해 클리핑
            a = np.clip(a, 1e-10, 1-1e-10)
            # 로그 손실과 규제 손실을 더하여 리스트에 추가
            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))
            self.losses.append((loss + self.reg_loss()) / m)
            # 검증 세트에 대한 손실 계산
            self.update_val_loss(x_val, y_val)

    def predict(self, x):
        z = self.forpass(x)      # 정방향 계산 수행
        return z > 0             # 스텝 함수 적용
    
    def score(self, x, y):
        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환
        return np.mean(self.predict(x) == y.reshape(-1, 1))
    
    def reg_loss(self):
        # 가중치에 규제 적용
        return self.l1 * np.sum(np.abs(self.w)) + self.l2 / 2 * np.sum(self.w**2)
    
    def update_val_loss(self, x_val, y_val):
        z = self.forpass(x_val)            # 정방향 계산 수행
        a = self.activation(z)             # 활성화 함수 적용
        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값 클리핑
        # 로그 손실과 규제 손실을 더하여 리스트에 추가
        val_loss = np.sum(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))
        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))
  ```
9. 훈련 데이터 표준화 전처리하기
   - 안정적인 학습을 위하여 데이터 세트의 특성을 평균이 0, 표준 편차가 1이 되도록 변환
   - 변환기(transformer) : 데이터 전처리에 관련된 클래스들 
   ```
   from sklearn.preprocessing import StandardScaler
   scaler = StandardScaler()                    # 객체 생성
   scaler.fit(x_train)                          # 변환 규칙 익히기
   x_train_scaled = scaler.transform(x_train)   # 데이터를 표준화 전처리
   x_val_scaled = scaler.transform(x_val)
   ```
10. 이 데이터를 `SingleLayer` 클래스 객체에 전달하여 배치 경사 하강법 적용
    ```
    single_layer = SingleLayer(l2=0.01)
    single_layer.fit(x_train_scaled, y_train, x_val=x_val_scaled, y_val=y_val, epochs=10000)
    single_layer.score(x_val_scaled, y_val)

    # 출력
    # 0.978021978021978
    ```
    - 05장 결과와 비교하기 위해 L2 규제 매개변수 값을 0.01로 지정
    - 에포크 매개변수의 기본값을 100에서 10,000으로 크게 늘림
      - **확률적 경사 하강법과 배치 경사 하강법은 에포크마다 가중치 업데이트를 하는 횟수에 차이가 있기 때문**
      - 확률 경사 하강법은 1개의 샘플을 이용하여 가중치를 업데이트 하므로, n개의 샘플이 있으면 1에포크당 n번의 가중치 업데이트 수행
      - 배치 경사 하강법은 전체 훈련 세트를 한 번에 계산한 다음 오차를 역전파하기 때문에, n개의 샘플이 있어도 1에포크당 1번의 가중치 업데이트만 수행
        - ex) 100번의 에포크를 수행하면 가중치는 딱 100번만 업데이트
      - 확률적 경사 하강법보다 **에포크 횟수를 크게 늘려줘야 함**
11. 검증 세트로 성능 측정하고 그래프로 비교하기
    - 훈련 손실과 검증 손실을 그래프로 출력
    ```
    plt.ylim(0, 0.3)
    plt.plot(single_layer.losses)
    plt.plot(single_layer.val_losses)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train_loss', 'val_loss'])
    plt.show()
    ```
    - `score()`에서 출력된 검증 세트의 점수는 05장과 동일
    - 손실 함수 값의 변화는 다름
      - 확률적 경사 하강법의 손실 그래프는 변동이 매우 심했음
      - 배치 경사 하강법은 전체 샘플을 사용하여 가중치를 업데이트하기 때문에 **손실값이 안정적으로 감소함**
12. 가중치의 변화 그래프
    ```
    w2 = []
    w3 = []
    for w in single_layer.w_history:
        w2.append(w[2])
        w3.append(w[3])
    plt.plot(w2, w3)
    plt.plot(w2[-1], w3[-1], 'ro')
    plt.xlabel('w[2]')
    plt.ylabel('w[3]')
    plt.show()
    ```
    - 가중치를 찾는 경로가 다소 부드러운 곡선의 형태를 띔
    - **가중치의 변화가 연속적이므로 손실값도 안정적으로 수렴**
    - 단점은 연산 비용이 많이 들고, 최솟값에 수렴하는 시간도 오래 걸림

<br>
<br>

### 6-2. 2개의 층을 가진 신경망 구현
- 지금까지는 하나의 층에 하나의 뉴런을 사용한 신경망 알고리즘으로 문제를 해결
- 지금부터는 하나의 층을 추가해보고, 층에 있는 뉴런 개수도 늘려볼 것

<br>

#### 하나의 층에 여러 개의 뉴런 사용
- 입력층 : 입력값이 나열된 부분
- ex) 3개의 특성과 2개의 뉴런이 있는 경우
  - 3개의 특성은 각각 2개의 뉴런에 모두 전달되어 2개의 출력값이 나옴
  - 입력층과 곱해지는 가중치가 각각 3개씩 필요하고, 뉴런마다 절편도 하나씩 필요함
- **여러 개의 뉴런을 사용함으로써 가중치가 1개의 열을 가진 벡터가 아닌, 2차원 행렬이 됨**
- **가중치 행렬의 크기**는 **(입력의 개수, 출력의 개수)**
  - 하나의 뉴런만 사용할 경우 출력의 개수가 1이므로 가중치는 열 벡터가 됨
- (책 p.170-171 참고) 

<br>

#### 출력을 하나로 모으기
- 위스콘신 유방암 데이터 세트로 해결할 문제는 악성 종양인지 정상 종양인지 구분하는 것
- 유방암 데이터 1개의 샘플에 있는 여러 특성의 값을 각 뉴런에 통과시키면 여러 개의 출력값이 나오는데, 이 값들 중 하나만 골라 이진 분류에 사용할 수는 없음
  - 각 뉴런의 **출력값들을 하나의 뉴런으로 다시 모아 이진 분류를 수행할 기준값(z)을 만들어야 함**
- 단일층 신경망 그림과 비슷함

<br>

#### 은닉층이 추가된 신경망
- **입력층 > 은닉층 > 출력층**
  - 입력값이 모여 있는 입력층은 층의 개수에 포함시키지 않음
- **입력과 출력은 행렬로 표기함**
  - 행렬을 사용하면 여러 개의 뉴런이 있는 다층 신경망을 쉽게 표현 가능
  - 신경망에서 **하나의 층을 행렬로, 하나의 뉴런을 행렬의 열**로 생각
    - 층을 많이 쌓은 신경망은 행렬의 개수가 많으므로 행렬 연산도 많이 수행해야 함
- (책 p.172 그림 참고) 

<br>

#### 다층 신경망 개념 정리
- 보통 다층 신경망의 각 층은 2개 이상의 뉴런으로 구성
- n개의 입력이 m개의 뉴런으로 입력되고, 은닉층을 통과한 값들은 다시 출력층으로 모임 -> 이것이 딥러닝
- **활성화 함수는 층마다 다를 수 있지만 한 층에서는 같아야 함**
  - 각 층은 하나 이상의 뉴런을 가짐
  - **은닉층과 출력층에 있는 모든 뉴런에는 활성화 함수가 필요**하며, 문제에 맞는 활성화 함수를 사용해야 함
  - 단, 같은 층에 있는 뉴런은 모두 같은 활성화 함수를 사용해야 함 
- **모든 뉴런이 연결되어 있으면 완전 연결(fully-connected) 신경망이라고 부름**
  - 인공 신경망의 한 종류이며, 가장 기본적인 신경망 구조
  - 완전 연결층 : 뉴런이 모두 연결되어 있는 층
  - 다층 퍼셉트론이라고도 부름

<br>

#### 다층 신경망에 경사 하강법 적용
- 책 p.175-181 다층 신경망의 구조와 수식 참고
- **다층 신경망에 경사 하강법을 적용하려면, 각 층의 가중치와 절편에 대하여 손실함수의 도함수를 구해야 함**
- **미분 순서는 출력층에서 은닉층 방향**
  1. 가중치에 대하여 손실 함수를 미분(출력층) 
  2. 도함수를 곱하기(출력층)
  3. 절편에 대하여 손실 함수를 미분(출력층)
  4. 가중치에 대하여 손실 함수를 미분(은닉층)
  5. 도함수를 곱하기(은닉층)
     - 신경망의 역방향 순서로 진행
  6. 절편에 대하여 손실 함수를 미분하고 도함수를 곱하기
- 미분의 연쇄 법칙을 적용하면 출력층에서부터 입력층까지 **오차를 거꾸로 전파하는 모습**을 제대로 볼 수 있음

<br>

#### 2개의 층을 가진 신경망 구현
- 은닉층 부분의 계산을 제외하면 비슷한 기능을 가지고 있으므로 **`SingleLayer` 클래스를 상속**하여 새로운 `DualLayer` 클래스를 만들고, 필요한 메서드만 재정의
1. `SingleLayer` 클래스를 상속한 `DualLayer` 클래스 만들기
   ```
   class DualLayer(SingleLayer):        # 상속
    
    def __init__(self, units=10, learning_rate=0.1, l1=0, l2=0):
        self.units = units         # 은닉층의 뉴런 개수
        self.w1 = None             # 은닉층의 가중치
        self.b1 = None             # 은닉층의 절편
        self.w2 = None             # 출력층의 가중치
        self.b2 = None             # 출력층의 절편
        self.a1 = None             # 은닉층의 활성화 출력(역방향 계산에 필요)
        self.losses = []           # 훈련 손실
        self.val_losses = []       # 검증 손실
        self.lr = learning_rate    # 학습률
        self.l1 = l1               # L1 손실 하이퍼파라미터
        self.l2 = l2               # L2 손실 하이퍼파라미터
   ```
2. `forpass()` 메서드 수정하기
   ```
    def forpass(self, x):
        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식 계산
        self.a1 = self.activation(z1)            # 활성화 함수 적용
        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식 계산
        return z2
   ```
   - **은닉층과 출력층의 정방향 계산 수행**
   - 은닉층의 활성화 함수를 통과한 a1과 출력층의 가중치 w2를 곱하고, b2를 더하여, 최종 출력 z2를 반환
   - `activation()` : `SingleLayer` 클래스로부터 상속 받음
3. `backprop()` 메서드 수정하기
   ```
    def backprop(self, x, err):
        m = len(x)       # 샘플 개수
        # 출력층의 가중치와 절편에 대한 그래디언트 계산
        w2_grad = np.dot(self.a1.T, err) / m
        b2_grad = np.sum(err) / m
        # 시그모이드 함수까지 그래디언트 계산
        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)
        # 은닉층의 가중치와 절편에 대한 그래디언트 계산
        w1_grad = np.dot(x.T, err_to_hidden) / m
        b1_grad = np.sum(err_to_hidden, axis=0) / m
        return w1_grad, b1_grad, w2_grad, b2_grad
   ```
   - 그레이디언트 계산하는 메서드, **모델 훈련에서 오차를 역전파하는 역할**
   - (책 p.183-184 수식 참고)
4. `fit()` 메서드 수정하기
   - **모델 훈련을 담당**하는 메서드
   - 해야 하는 작업
     - 은닉층과 출력층의 가중치, 절편을 초기화하고 에포크마다 정방향 계산을 수행하여 오차 계산
     - 오차를 역전파하여 가중치와 절편의 그레이디언트 계산
     - 손실을 계산하여 누적
   - `DualLayer` 클래스의 `fit()` 은 3개의 작은 메서드로 분리  
5. `fit()` 메서드에 있던 가중치 초기화 부분을 `init_weights()` 메서드로 분리
   ```
    def init_weights(self, n_features):
        self.w1 = np.ones((n_features, self.units))  # (입력 특성 개수, 은닉층의 크기)
        self.b1 = np.zeros(self.units)               # 은닉층의 크기
        self.w2 = np.ones((self.units, 1))           # (은닉층의 크기, 1)
        self.b2 = 0
   ```
6. `fit()` 메서드의 for문 안에 있는 코드 중 일부를 `training()` 메서드로 분리
   ```
    def fit(self, x, y, epochs=100, x_val=None, y_val=None):
        y = y.reshape(-1, 1)          # 타깃을 열 벡터로 바꿈
        y_val = y_val.reshape(-1, 1)
        m = len(x)                    # 샘플 개수 저장
        self.init_weights(x.shape[1]) # 은닉층과 출력층의 가중치 초기화
        # epochs만큼 반복
        for i in range(epochs):
            a = self.training(x, y, m)
            # 안전한 로그 계산을 위해 클리핑
            a = np.clip(a, 1e-10, 1-1e-10)
            # 로그 손실과 규제 손실을 더하여 리스트에 추가
            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))
            self.losses.append((loss + self.reg_loss()) / m)
            # 검증 세트에 대한 손실 계산
            self.update_val_loss(x_val, y_val)
            

    def training(self, x, y, m):
        z = self.forpass(x)       # 정방향 계산 수행
        a = self.activation(z)    # 활성화 함수 적용
        err = -(y - a)            # 오차 계산
        # 오차를 역전파하여 그래디언트 계산
        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        # 그래디언트에 페널티 항의 미분 값 더하기
        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m
        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m
        # 은닉층의 가중치와 절편 업데이트
        self.w1 -= self.lr * w1_grad
        self.b1 -= self.lr * b1_grad
        # 출력층의 가중치와 절편 업데이트
        self.w2 -= self.lr * w2_grad
        self.b2 -= self.lr * b2_grad
        return a
   ```
   - 정방향 계산과 그레이디언트를 업데이트하는 코드를 `training()`으로 옮김
   - 훈련 데이터 x, y와 훈련 샘플의 개수 m을 매개변수로 받고, 마지막 출력층의 활성화 출력 a 반환
7. `reg_loss()` 메서드 수정하기
   ```
    def reg_loss(self):
        # 은닉층과 출력층의 가중치에 규제를 적용
        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \
               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))
   ```
   - 은닉층과 출력층의 가중치에 대한 **L1, L2 손실 계산**하는 메서드

<br>

#### 모델 훈련하기
- `DualLayer` 클래스로 다층 신경망 모델 훈련
1. 다층 신경망 모델 훈련하고 평가하기
   ```
   dual_layer = DualLayer(l2=0.01)
   dual_layer.fit(x_train_scaled, y_train, x_val=x_val_scaled, y_val=y_val, epochs=20000)
   dual_layer.score(x_val_scaled, y_val)

   # 출력
   # 0.978021978021978
   ```
   - L2 규제는 0.01만큼, 에포크는 20,000번으로 지정
   - 평가 점수는 `SingleLayer` 클래스로 훈련한 모델(L2 규제 0.01, 에포크 10,000)과 동일함
     - 해결하려는 문제가 간단하기 때문, `DualLayer`의 성능이 더 좋음 
2. 훈련 손실과 검증 손실 그래프 분석하기
   ```
   plt.ylim(0, 0.3)
   plt.plot(dual_layer.losses)
   plt.plot(dual_layer.val_losses)
   plt.ylabel('loss')
   plt.xlabel('epoch')
   plt.legend(['train_loss', 'val_loss'])
   plt.show()
   ```
   - 훈련 손실 그래프 : 훈련 데이터로 손실 함수의 최솟값을 찾아가는 과정을 보여줌
   - 검증 손실 그래프 : 검증 데이터로 손실 함수의 최솟값을 찾아가는 과정을 보여줌
   - **손실 그래프가 이전보다 천천히 감소함**
     - **가중치의 개수가 이전보다 많아져서** 학습하는 데 시간이 오래 걸리기 때문
     - `SingleLayer` 클래스 : 가중치 30개와 절편 1개 필요
     - `DualLayer` 클래스 : 은닉층의 뉴런이 10개였으므로 30*10개의 가중치와 10개의 절편 필요, 출력층 역시 10개의 가중치와 1개의 절편 필요
   
<br>

#### 가중치 초기화 개선하기
- 손실 그래프의 **초기 손실값이 감소하는 곡선이 매끄럽지 않음**
  - 손실 함수가 감소하는 방향을 올바르게 찾는 데 많은 시간 소요된 것, 이는 **가중치 초기화와 관련 깊음**
  - 이번에는 1이 아니라, 넘파이의 `random.normal()` 함수를 사용하여 **정규 분포를 따르는 무작위 수로 가중치 초기화**  
1. 가중치 초기화를 위한 `init_weights()` 메서드 수정하기
   ```
   class RandomInitNetwork(DualLayer):      # DualLayer 클래스 상속
    
    def init_weights(self, n_features):
        np.random.seed(42)
        self.w1 = np.random.normal(0, 1, (n_features, self.units))  # (특성 개수, 은닉층의 크기)
        self.b1 = np.zeros(self.units)                              # 은닉층의 크기
        self.w2 = np.random.normal(0, 1, (self.units, 1))           # (은닉층의 크기, 1)
        self.b2 = 0
   ```
   - `normal()` 매개변수 : 순서대로 평균, 표준 편차, 배열 크기
2. `RandomInitNetwork` 클래스 객체를 다시 만들고, 모델 훈련  후 손실 함수 그리기
   ```
   random_init_net = RandomInitNetwork(l2=0.01)
   random_init_net.fit(x_train_scaled, y_train, x_val=x_val_scaled, y_val=y_val, epochs=500)
    
   plt.plot(random_init_net.losses)
   plt.plot(random_init_net.val_losses)
   plt.ylabel('loss')
   plt.xlabel('epoch')
   plt.legend(['train_loss', 'val_loss'])
   plt.show()
   ```
   - 손실 함수가 **감소하는 곡선이 매끄러워짐**
   - 가중치를 모두 1로 초기화한 것보다 훨씬 빠르게 손실 함수 값이 줄어들음
   - => **가중치를 무작위 수로 초기화한 것이 학습 성능에 영향을 미친 것**

<br>
<br>

### 6-3. 미니 배치를 사용하여 모델 훈련
- 실전에서는 확률적 경사 하강법과 배치 경사 하강법의 장점을 절충한 '미니 배치 경사 하강법'이 널리 사용

<br>

#### 미니 배치 경사 하강법
- 구현은 배치 경사 하강법과 비슷하지만 에포크마다 전체 데이터를 사용하는 것이 아니라, **조금씩 나누어 정방향 계산을 수행**하고 그레이디언트를 구하여 가중치를 업데이트 함
- 작게 나눈 **미니 배치만큼 가중치를 업데이트**
- 미니 배치의 크기는 
  - 보통 16, 32, 64 등 2의 배수 사용
  - 크기가 1 -> 확률적 경사 하강법
  - 크기가 전체 데이터를 포함하는 크기 -> 배치 경사 하강법
  - 즉, **크기에 따라 앞에서 배운 방법들의 장점 혹은 단점을 가지는 방식**
- 미니 배치의 최적값은 정해진 것이 아님 
- **미니 배치의 크기도 하이퍼파라미터이고 튜닝의 대상**

<br>

#### 미니 배치 경사 하강법 구현
1. `MinibatchNetwork` 클래스 구현하기
   ```
   class MinibatchNetwork(RandomInitNetwork):         # 상속
    
    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):
        super().__init__(units, learning_rate, l1, l2)
        self.batch_size = batch_size                  # 배치 크기
   ```
   - 새로 추가된 `batch_size` 매개변수만 직접 관리하고, 나머지 매개변수들은 `RandomInitNetwork` 의 `__init__()` 메서드로 전달 
2. `fit()` 메서드 수정하기
   ```
    def fit(self, x, y, epochs=100, x_val=None, y_val=None):
        y_val = y_val.reshape(-1, 1)     # 타깃을 열 벡터로 바꿈
        self.init_weights(x.shape[1])    # 은닉층과 출력층의 가중치 초기화
        np.random.seed(42)
        # epochs만큼 반복
        for i in range(epochs):
            loss = 0
            # 제너레이터 함수에서 반환한 미니배치 순환
            for x_batch, y_batch in self.gen_batch(x, y):
                y_batch = y_batch.reshape(-1, 1) # 타깃을 열 벡터로 바꿈
                m = len(x_batch)                 # 샘플 개수 저장
                a = self.training(x_batch, y_batch, m)
                # 안전한 로그 계산을 위해 클리핑
                a = np.clip(a, 1e-10, 1-1e-10)
                # 로그 손실과 규제 손실을 더하여 리스트에 추가
                loss += np.sum(-(y_batch*np.log(a) + (1-y_batch)*np.log(1-a)))
            self.losses.append((loss + self.reg_loss()) / len(x))
            # 검증 세트에 대한 손실 계산
            self.update_val_loss(x_val, y_val)
   ```
   - 에포크를 순회하는 for문 안에 **미니 배치를 순회하는 for문 추가**
   - `gen_batch()` 메서드
     - 전체 훈련 데이터 x, y를 전달받아 `batch_size`만큼 미니 배치를 만들어 반환
     - 반환된 미니 배치 데이터 x_batch, y_batch를 `training()` 에 전달
     - 간단하게 미니 배치 방식 구현 가능!
3. `gen_batch()` 메서드 만들기
   ```
    # 미니배치 제너레이터 함수
    def gen_batch(self, x, y):
        length = len(x)
        bins = length // self.batch_size # 미니배치 횟수
        if length % self.batch_size:
            bins += 1                    # 나누어 떨어지지 않을 때
        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞기
        x = x[indexes]
        y = y[indexes]
        for i in range(bins):
            start = self.batch_size * i
            end = self.batch_size * (i + 1)
            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환
   ```
   - `gen_batch()` : **파이썬 제너레이터(generator)로 구현**
     - 순차적으로 데이터에 접근할 수 있는 반복 가능한 객체를 반환
     - 명시적으로 리스트를 만들지 않으면서, 필요한 만큼 데이터 추출 가능하여 메모리 효율적으로 사용
     - 보통의 함수에 **yield문**을 사용하여 간단하게 만들 수 있음 
   - `MinibatchNetwork` 클래스 구현 완료 
4. 미니 배치 경사 하강법 적용하기
   - `batch_size`를 기본값 32로 하여 훈련
   ```
   minibatch_net = MinibatchNetwork(l2=0.01, batch_size=32)
   minibatch_net.fit(x_train_scaled, y_train, x_val=x_val_scaled, y_val=y_val, epochs=500)
   minibatch_net.score(x_val_scaled, y_val)

   # 출력
   # 0.978021978021978


   plt.plot(minibatch_net.losses)
   plt.plot(minibatch_net.val_losses)
   plt.ylabel('loss')
   plt.xlabel('iteration')
   plt.legend(['train_loss', 'val_loss'])
   plt.show()
   ```
   - 06-2절의 마지막 그래프와 비교하면, 미니 배치 경사 하강법은 에포크마다 훈련 반복이 여러 번 일어나므로 배치 경사 하강법보다 수렴 속도가 빨라진 것을 알 수 있음
5. 미니 배치 크기를 늘려서 다시 시도
   - `batch_size`를 기본값 128로 늘려 훈련
   ```
   minibatch_net = MinibatchNetwork(l2=0.01, batch_size=128)
   minibatch_net.fit(x_train_scaled, y_train, x_val=x_val_scaled, y_val=y_val, epochs=500)
   minibatch_net.score(x_val_scaled, y_val)

   # 출력
   # 0.978021978021978


   plt.plot(minibatch_net.losses)
   plt.plot(minibatch_net.val_losses)
   plt.ylabel('loss')
   plt.xlabel('iteration')
   plt.legend(['train_loss', 'val_loss'])
   plt.show()
   ```
- 128로 늘렸더니 손실 그래프는 조금 더 안정적으로 바뀌었지만, 손실값이 줄어드는 속도가 느려짐
- **일반적으로 미니 배치의 크기는 32-512개 사이의 값을 지정**

<br>

#### 사이킷런을 사용해 다층 신경망 훈련
- 사이킷런을 사용하여 앞에서 만든 다층 완전 연결 신경망 훈련
- 사이킷런은 `sklearn.neural_network` 모듈 아래에 
  - 분류 작업을 위한 : `MLPClassfier` 제공
  - 회귀 작업을 위한 : `MLPRegressor` 제공
1. `MLPClassifier`의 객체 만들기
   ```
   from sklearn.neural_network import MLPClassifier
   mlp = MLPClassifier(hidden_layer_sizes=(10, ), activation='logistic',
                    solver='sgd', alpha=0.01, batch_size=32, learning_rate_init=0.1, max_iter=1000)
   ```
   - **`MLPClassifier`의 주요 매개변수**
     - 은닉층의 크기를 정의하는 `hidden_layer_sizes`
     - 활성화 함수를 지정하는 `activation`
     - 경사 하강법 알고리즘의 종류를 지정하는 매개변수 `solver`
     - 규제를 적용하기 위한 매개변수 `alpha`
     - 배치 크기, 학습률 초깃값, 에포크 횟수를 정하는 매개변수 `batch_size`, `learning_rate_init`, `max_iter`
2. 모델 훈련하기
   ```
   mlp.fit(x_train_scaled, y_train)         # 스케일이 조정된 훈련 세트로 모델 훈련
   mlp.score(x_val_scaled, y_val)           # 검증 세트를 사용하여 정확도 평가

   # 출력
   # 0.989010989010989
   ```
   - 앞에서 직접 만든 모델보다 조금 향상된 성능
     - `MLPClassifier` 구현은 직접 만든 `MinibatchNetwork` 클래스와는 기술적으로 미묘한 차이가 있기 때문