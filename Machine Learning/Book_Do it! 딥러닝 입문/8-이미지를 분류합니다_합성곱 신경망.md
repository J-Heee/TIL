# Chapter 8. 이미지를 분류합니다 - 합성곱 신경망

## 2021.02.25 - 2021.03.01

### 8-1. 합성곱(convolution) 연산

#### 합성곱을 그림으로 이해하기
- 합성곱은 두 함수에 적용하여 새로운 함수를 만드는 수학 연산자
- 책 p.234-235 그림 참고
1. 배열 하나 선택해 뒤집기
   - 두 배열 x와 w가 있다고 가정하고, 두 배열 중 **원소수가 적은 배열 w의 원소 순서를 뒤집기**
   - 뒤집은 배열은 reverse의 약자 r을 사용하여 **w<sup>r</sup>**
2. 첫 번째 합성곱
   - **뒤집은 배열을 배열 x의 왼쪽 끝자리에** 맞춰 배치
   - **점 곱 연산**(각 배열 원소끼리 곱한 후 더하는 연산) 수행
3. 두 번째 합성곱
   - **w<sup>r</sup>을 오른쪽으로 한 칸 이동**하여, 다시 점 곱 연산 수행
4. 나머지 합성곱
   - 같은 방식으로 w<sup>r</sup>을 오른쪽으로 한 칸씩 이동하여, **x의 끝에 도착할 때까지** 합성곱 수행
   - **합성곱은 수식으로 `x * w`** 와 같이 표기 (파이썬의 곱셈 연산자와 같은 기호로 표기하지만, 다른 연산 의미)

<br>

#### 합성곱 구현하기
1. **넘파이 배열 정의하고 배열 하나 선택해 뒤집기**
   ```
   import numpy as np
   w = np.array([2, 1, 5, 3])
   x = np.array([2, 8, 3, 7, 1, 2, 0, 4, 5])
   ```
   - 넘파이의 `flip()` 함수로 배열 뒤집기 가능
   ```
   w_r = np.flip(w)
   print(w_r)

   # 출력
   # [3 5 1 2]
   ```
   - 파이썬의 슬라이스 연산자를 이용해도 배열 뒤집기 가능
   ```
   w_r = w[::-1]
   print(w_r)

   # 출력
   # [3 5 1 2]
   ```
2. **넘파이의 점 곱으로 합성곱 수행하기**
   - x배열을 한 칸씩 이동하면서 넘파이의 점 곱을 이용하여 합성곱 수행
   ```
   for i in range(6):
      print(np.dot(x[i:i+4], w_r))

   # 출력
   # 63
     48
     49
     28
     21
     20
   ```
3. **싸이파이로 합성곱 수행하기**
   - 싸이파이는 합성곱을 위한 함수 `convolve()` 제공
   - `mode` 매개변수에 대해서는 뒤에서 자세히 알아볼 예정
   ```
   from scipy.signal import convolve
   convolve(x, w, mode='valid')

   # 출력
   # array([63, 48, 49, 28, 21, 20])
   ```

<br>

#### 합성곱 신경망은 진짜 합성곱을 사용하지 않는다
- 사실 대부분의 딥러닝 패키지들은 합성곱 신경망을 만들 때, **합성곱이 아니라 교차 상관을 사용**
- **합성곱과 교차 상관(cross-correlation)은 아주 비슷**
  - 교차 상관은 합성곱과 동일한 방법으로 연산이 진행되지만, **'미끄러지는 배열을 뒤집지 않는다'** 는 차이점 (책 p.237 그림 참고)
  - 교차 상관도 **싸이파이의 `correlate()` 함수**로 간단히 계산 가능
  ```
  from scipy.signal import correlate
  correlate(x, w, mode='valid')

  # 출력
  # array([48, 57, 24, 25, 16, 39])
  ```
- **합성곱 신경망에서 교차 상관을 사용하는 이유**
  - 모델 훈련 과정 간단 정리
    - 가중치를 무작위 값으로 초기화
    - 모든 샘플에 대하여 정방향과 역방향 계산을 수행하여 가중치를 조금씩 학습(업데이트)
  - 모든 모델과 마찬가지로, 합성곱 신경망으로 만든 모델도 훈련하기 전에 가중치 배열의 요소들을 무작위로 초기화
  - 합성곱에서 예시로 든 '미끄러지는 배열'이 가중치 배열에 해당
  - **가중치 배열은 무작위로 초기화되어 있으므로, 가중치를 뒤집어서 합성곱을 적용하던지, 뒤집지않고 교차 상관을 적용하던지 상관이 없음**
  - 하지만, 합성곱 신경망이라는 이름이 관례적으로 널리 사용되고 있으므로 개념과 용어 혼동하지 않기

<br>

#### 패딩(padding)
- **패딩 : 원본 배열의 양 끝에 빈 원소를 추가하는 것**
- 패딩과 스트라이드가 어떻게 적용되는지에 따라 밸리드 패딩, 풀 패딩, 세임 패딩이라고 부름
- **밸리드 패딩(valid padding)**
  - 앞에서 교차 상관을 싸이파이로 구현할 때, **`mode` 매개변수에 `valid`를 지정한 것**이 바로 밸리드 패딩을 적용한 예
  - **원본 배열에 패딩을 추가하지 않고**, 미끄러지는 배열이 원본 배열의 끝으로 갈 때까지 교차 상관을 수행
    - 따라서 밸리드 패딩의 결과로 얻는 배열의 크기는 원본 배열보다 항상 작음
  - **원본 배열의 각 원소가 합성곱 연산에 참여하는 정도가 서로 다름**
    - 원본 배열 양 끝 원소의 연산 참여도가 낮음 (책 p.238 그림 참고)
- **풀 패딩(full padding)**
  - **원본 배열의 모든 요소가 동일하게 연산에 참여하는 패딩 방식** 
  - 원본 배열의 원소가 연산에 동일하게 참여하려면, **원본 배열의 양 끝에 가상의 원소를 추가**해야 함
    - 가상의 원소로 **0을 사용**하기 때문에, 이를 **제로 패딩(zero padding)** 이라고 부름
  - 적절한 개수의 제로 패딩을 추가하면 원본 배열의 모든 원소가 연산에 동일하게 참여 가능 (책 p.239 그림 참고)
  - `correlate()` 함수에서 **매개변수 `mode`를 `full`로** 지정하여 풀 패딩 적용
    ```
    correlate(x, w, mode='full')

    # 출력
    # array([ 6, 34, 51, 48, 57, 24, 25, 16, 39, 29, 13, 10])
    ```
- **세임 패딩(same padding)**
  - **출력 배열의 길이가 원본 배열의 길이와 동일해지도록** 원본 배열에 제로 패딩 추가
  - `correlate()` 함수에서 **매개변수 `mode`를 `same`으로** 지정하여 세임 패딩 적용
    ```
    correlate(x, w, mode='same')

    # 출력
    # array([34, 51, 48, 57, 24, 25, 16, 39, 29])
    ```
- **합성곱 신경망에서는 대부분 세임 패딩을 사용**

<br>

#### 스트라이드(stride)
- **스트라이드 : 미끄러지는 배열의 간격을 조절하는 것**
- 지금까지 사용한 밸리드 패딩, 풀 패딩, 세임 패딩은 모두 1칸씩 미끄러지며 연산 수행
  - 스트라이드를 1로 지정하여 연산 수행된 것
- 스트라이드를 2로 지정하면, 2칸씩 미끄러지며 연산 수행
- **합성곱 신경망을 만들 때는 보통 스트라이드를 1로 지정**

<br>

#### 2차원 배열에서 합성곱 수행
- 지금까지는 이해를 위해 1차원 배열을 사용하여 합성곱, 패딩, 스트라이드를 알아보았지만, **합성곱 신경망은 대부분 2차원 배열에 대한 합성곱을 사용**
- 2차원 배열의 합성곱도 1차원 배열의 합성곱과 비슷하게 수행
- 합성곱의 수행 방향은
  - 원본 배열의 **왼쪽에서 오른쪽으로**,
  - **위에서 아래쪽으로 1칸씩 이동**하며 배열 원소끼리 곱하는 것
  - 책 p.241 그림 참고
    - 2차원 원본 배열 x, 미끄러지는 배열 w
    - 원본 배열의 왼쪽 모서리 끝에 미끄러지는 배열을 맞추고 합성곱 수행
    - 미끄러지는 배열을 오른쪽으로 1칸 옮겨서 합성곱 수행
    - 오른쪽 끝에 도달하면, 아래로 1칸 내리고 다시 왼쪽 끝부터 합성곱 수행
    - 그림의 원본 배열의 크기는 3*3이므로, 밸리드 패딩을 하면 미끄러지는 배열이 이동하는 횟수는 총 4번 
- **싸이파이의 `correlate2d()` 함수로 2차원 배열의 합성곱 계산**
  ```
  x = np.array([[1, 2, 3],
                [4, 5, 6],
                [7, 8, 9]])
  w = np.array([[2, 0], [0, 0]])

  from scipy.signal import correlate2d
  correlate2d(x, w, mode='valid')

  # 출력
  # array([[ 2,  4],
           [ 8, 10]])
  ```
- **2차원 배열에서의 세임 패딩**
  - **오른쪽과 아래쪽 모서리에 제로 패딩 추가**
  - 세임 패딩 적용 (책 p.242 그림 참고)
  ```
  correlate2d(x, w, mode='same')

  # 출력
  # array([[ 2,  4,  6],
           [ 8, 10, 12],
           [14, 16, 18]])
  ```
    - 원본 배열의 크기와 같은 출력 배열 생성
- **2차원 배열에서의 스트라이드**
  - 미끄러지는 방향은 그대로 유지하면서, 미끄러지는 **간격의 크기만 증가**

<br>

#### 텐서플로로 합성곱 수행
- 지금까지는 합성곱을 위해 싸이파이를 사용
- 텐서플로에서 합성곱을 위해 제공하는 함수를 사용하여 싸이파이와 동일한 결과를 출력하는지 비교
- 합성곱 신경망이 기준이므로 **원본 배열을 입력**, **미끄러지는 배열을 가중치**라고 부를 것
- **합성곱 신경망의 입력은 일반적으로 4차원 배열**
  - **텐서플로에서 2차원 합성곱을 수행하는 함수는 `conv2d()`로, 입력으로 4차원 배열을 기대함**
    - 입력 이미지의 높이와 너비 외에 더 많은 차원이 필요하기 때문
  - **4차원 입력 배열**의 구성 (책 p.243 참고)
    - 입력에 2개의 샘플이 포함되어 있으며
    - 각 샘플은 R, G, B로 구분되는 3개의 컬러 채널을 가짐
    - 그림의 입력을 4차원 배열로 표현하면 (2, 3, 3, 3)
      - **배치, 샘플의 높이, 샘플의 너비, 컬러 채널의 차원** 의미
  - 입력과 곱해지는 가중치도 4개의 차원으로 구성
    - 그림의 경우를 배열로 표현하면 (2, 2, 3, 3)
      - **가중치의 높이, 너비, 채널, 가중치의 개수** 의미
  - 입력과 가중치에 **세임 패딩**을 적용하여 합성곱 수행하면, **(입력의 배치, 입력의 높이, 입력의 너비, 가중치의 개수)** 가 됨
    - 책 p.244 그림 참고
    - 일반적인 합성곱의 입력과 가중치의 채널 수는 동일
    - 즉, 채널 방향으로는 가중치가 이동하지 않음
- **텐서플로를 활용하여 2차원 배열을 4차원 배열로 바꿔 합성곱 수행**
  - 넘파이의 **`reshape()`** 메서드로, 입력 x와 가중치 w를 2차원 배열에서 **4차원 배열로 변환**
  - 넘파이의 **`astype()`** 메서드로, 텐서플로는 실수형의 입력을 기대하므로 **입력의 자료형을 실수로 변환**
  - 배치와 컬러 채널은 1
   ```
   import tensorflow as tf
   x_4d = x.astype(np.float).reshape(1, 3, 3, 1)
   w_4d = w.reshape(2, 2, 1, 1)
   ```
   - 스트라이드는 1 적용
   - 패딩은 세임 패딩 적용
   - 텐서플로의 `conv2d()` 함수의 패딩 옵션은 대문자 사용
   ```
   c_out = tf.nn.conv2d(x_4d, w_4d, strides=1, padding='SAME')
   ```
   - **`conv2d()` 함수는 결과값으로 텐서플로의 `Tensor` 객체를 반환**
     - **텐서(tensor) : 텐서플로에서는 다차원 배열을 텐서라고 부름**
   - Tensor 객체의 numpy() 메서드를 사용하면 텐서를 넘파이 배열로 변환 가능
   - 배치 차원과 컬러 차원을 제거하고 편의상 (3, 3) 크기로 변환하여 출력
   ```
   c_out.numpy().reshape(3, 3)

   # 출력
   # array([[ 2.,  4.,  6.],
            [ 8., 10., 12.],
            [14., 16., 18.]])
   ```
   - 2차원 배열로 실습했던 `correlate2d()`의 출력 결과와 동일하지만, 실제 **텐서플로의 `conv2d()` 함수에 전달되는 매개변수의 값은 4차원 배열**임을 잊지 말기

<br>

#### 패션 MNIST 데이터 세트를 합성곱 신경망에 적용하면
- 07장에서는 패션 MNIST 데이터 세트를 `MultiClassNetwork` 클래스에 적용하며 28 * 28 크기의 입력을 일렬로 펼쳐서 사용
  - 따라서 가중치의 개수도 많이 필요했음
- **합성곱 신경망에서는 28 * 28 크기의 입력을** 펼치지 않고 **그대로 사용**
  - 따라서 3 * 3 또는 5 * 5 크기의 가중치로 합성곱 적용
  - 가중치 배열의 크기는 훨씬 작아졌고, 입력의 특징을 더 잘 찾기 때문에 **합성곱 신경망이 이미지 분류에서 뛰어난 성능 발휘 가능**

<br>

#### 합성곱의 가중치를 필터(filter) 또는 커널(kernel)이라고 부름
- 이 책에서는
  - 합성곱의 **필터 1개**를 지칭할 때는 **'커널'**
  - 합성곱의 **필터 전체**를 지칭할 때는 일반 신경망과 동일하게 **'가중치'**

<br>
<br>

### 8-2. 풀링 연산
- 합성곱 신경망에서는
  - **합성곱층 : 합성곱이 일어나는 층**
  - **풀링층 : 풀링이 일어나는 층**
  - **특성 맵(feature map) : 합성곱층과 풀링층에서 만들어진 결과**
- **합성곱층 뒤에 풀링층이 뒤따르는 형태**는 합성곱 신경망의 전형적인 모습
  - 입력이 합성곱층을 통과할 때, 합성곱과 활성화 함수가 적용되어 특성 맵이 만들어짐
  - 특성 맵이 풀링층을 통과하여 또 다른 특성 맵이 만들어짐
- **풀링(pooling) : 특성 맵을 스캔하며 최댓값을 고르거나 평균값을 계산하는 것**
- 합성곱 신경망에서는 최대 풀링과 평균 풀링을 주로 사용

<br>

#### 최대 풀링(max pooling)
- **최댓값을 고르는 풀링 방식**
- **풀링 영역의 크기는 보통 2 * 2**를 지정
- **스트라이드는** 일반적으로 **풀링의 한 모서리 크기**로 지정
  - 즉, 스트라이드는 2를 적용
  - 풀링 영역이 겹쳐지지 않도록 스캔
  - (책 p.247 그림 참고)
- **2 * 2 풀링은 특성 맵의 크기를 절반으로 줄임** (면적은 1/4)
  - **특성 맵의 한 요소가 입력의 더 넓은 영역을 바라볼 수 있는 효과**
  - 합성곱층에서 스트라이드를 크게 지정하여 특성 맵의 크기를 줄이면 안되나?
    - 경험적으로 합성곱층에 세임 패딩을 적용하고, 풀링층에서 특성 맵의 크기를 줄이는 것이 더 효과적

<br>

#### 평균 풀링(average pooling)
- **풀링 영역의 평균값을 계산하는 방식**
  - (책 p.248 그림 참고)
- **연구자들은** 보통 평균 풀링보다 **최대 풀링을 선호함**
  - 평균 풀링은 합성곱층을 통과하는 특징들을 희석시킬 가능성이 높기 때문
    - 입력에서 합성곱 필터가 찾고자 하는 부분은, 특성 맵의 가장 큰 값으로 활성화되는데, 평균 풀링은 가장 큰 특성의 값을 상쇄시킴
  - **최대 풀링은 가장 큰 특징을 유지시키는 성질**
    - 이미지 분류 작업에 적합

<br>

#### 최대 풀링과 평균 풀링 수행
- **텐서플로의 `max_pool2d()` 함수** 사용
  - 매개변수 값으로 풀링 크기와 스트라이드만 전달하면, 자동으로 **최대 풀링 수행**
  ```
  x = np.array([[1, 2, 3, 4],
                [5, 6, 7, 8],
                [9, 10, 11, 12],
                [13, 14, 15, 16]])
  x = x.reshape(1, 4, 4, 1)
  ```
  - 1-16의 값이 들어있는 4 * 4 크기의 배열 생성
  - 1 * 4 * 4 * 1 크기의 배열로 변형
    - `max_pool2d()` 함수는 `conv2d()` 함수와 마찬가지로, 첫 번째 차원으로 배치 차원을 기대하기 때문
  ```
  p_out = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')
  p_out.numpy().reshape(2, 2)

  # 출력
  # array([[ 6.,  8.],
           [14., 16.]])
  ```
  - **`ksize`** 매개변수 : 풀링의 크기 지정
  - **`strides`** 매개변수 : 스트라이드 크기 지정
  - `max_pool2d()` 함수가 반환한 `Tensor` 객체를 `numpy()` 메서드로 변환 후, 2 * 2 크기의 2차원 배열로 변형
  - 출력 결과를 통해 **성공적으로 최대 풀링**이 된 것 확인 가능
- 기억할 것
  - **풀링층에는 학습되는 가중치가 없음**
  - 풀링은 배치 차원이나 채널 차원으로 적용되지 않음
    - **풀링층을 통과하기 전후로 배치 크기와 채널 크기는 동일**
    - 풀링은 각 샘플마다, 각 채널마다 독립적으로 수행

<br>

