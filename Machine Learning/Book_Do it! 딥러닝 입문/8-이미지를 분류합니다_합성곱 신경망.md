# Chapter 8. 이미지를 분류합니다 - 합성곱 신경망

## 2021.02.25 - 2021.03.02

### 8-1. 합성곱(convolution) 연산

#### 합성곱을 그림으로 이해하기
- 합성곱은 두 함수에 적용하여 새로운 함수를 만드는 수학 연산자
- 책 p.234-235 그림 참고
1. 배열 하나 선택해 뒤집기
   - 두 배열 x와 w가 있다고 가정하고, 두 배열 중 **원소수가 적은 배열 w의 원소 순서를 뒤집기**
   - 뒤집은 배열은 reverse의 약자 r을 사용하여 **w<sup>r</sup>**
2. 첫 번째 합성곱
   - **뒤집은 배열을 배열 x의 왼쪽 끝자리에** 맞춰 배치
   - **점 곱 연산**(각 배열 원소끼리 곱한 후 더하는 연산) 수행
3. 두 번째 합성곱
   - **w<sup>r</sup>을 오른쪽으로 한 칸 이동**하여, 다시 점 곱 연산 수행
4. 나머지 합성곱
   - 같은 방식으로 w<sup>r</sup>을 오른쪽으로 한 칸씩 이동하여, **x의 끝에 도착할 때까지** 합성곱 수행
   - **합성곱은 수식으로 `x * w`** 와 같이 표기 (파이썬의 곱셈 연산자와 같은 기호로 표기하지만, 다른 연산 의미)

<br>

#### 합성곱 구현하기
1. **넘파이 배열 정의하고 배열 하나 선택해 뒤집기**
   ```
   import numpy as np
   w = np.array([2, 1, 5, 3])
   x = np.array([2, 8, 3, 7, 1, 2, 0, 4, 5])
   ```
   - 넘파이의 `flip()` 함수로 배열 뒤집기 가능
   ```
   w_r = np.flip(w)
   print(w_r)

   # 출력
   # [3 5 1 2]
   ```
   - 파이썬의 슬라이스 연산자를 이용해도 배열 뒤집기 가능
   ```
   w_r = w[::-1]
   print(w_r)

   # 출력
   # [3 5 1 2]
   ```
2. **넘파이의 점 곱으로 합성곱 수행하기**
   - x배열을 한 칸씩 이동하면서 넘파이의 점 곱을 이용하여 합성곱 수행
   ```
   for i in range(6):
      print(np.dot(x[i:i+4], w_r))

   # 출력
   # 63
     48
     49
     28
     21
     20
   ```
3. **싸이파이로 합성곱 수행하기**
   - 싸이파이는 합성곱을 위한 함수 `convolve()` 제공
   - `mode` 매개변수에 대해서는 뒤에서 자세히 알아볼 예정
   ```
   from scipy.signal import convolve
   convolve(x, w, mode='valid')

   # 출력
   # array([63, 48, 49, 28, 21, 20])
   ```

<br>

#### 합성곱 신경망은 진짜 합성곱을 사용하지 않는다
- 사실 대부분의 딥러닝 패키지들은 합성곱 신경망을 만들 때, **합성곱이 아니라 교차 상관을 사용**
- **합성곱과 교차 상관(cross-correlation)은 아주 비슷**
  - 교차 상관은 합성곱과 동일한 방법으로 연산이 진행되지만, **'미끄러지는 배열을 뒤집지 않는다'** 는 차이점 (책 p.237 그림 참고)
  - 교차 상관도 **싸이파이의 `correlate()` 함수**로 간단히 계산 가능
  ```
  from scipy.signal import correlate
  correlate(x, w, mode='valid')

  # 출력
  # array([48, 57, 24, 25, 16, 39])
  ```
- **합성곱 신경망에서 교차 상관을 사용하는 이유**
  - 모델 훈련 과정 간단 정리
    - 가중치를 무작위 값으로 초기화
    - 모든 샘플에 대하여 정방향과 역방향 계산을 수행하여 가중치를 조금씩 학습(업데이트)
  - 모든 모델과 마찬가지로, 합성곱 신경망으로 만든 모델도 훈련하기 전에 가중치 배열의 요소들을 무작위로 초기화
  - 합성곱에서 예시로 든 '미끄러지는 배열'이 가중치 배열에 해당
  - **가중치 배열은 무작위로 초기화되어 있으므로, 가중치를 뒤집어서 합성곱을 적용하던지, 뒤집지않고 교차 상관을 적용하던지 상관이 없음**
  - 하지만, 합성곱 신경망이라는 이름이 관례적으로 널리 사용되고 있으므로 개념과 용어 혼동하지 않기

<br>

#### 패딩(padding)
- **패딩 : 원본 배열의 양 끝에 빈 원소를 추가하는 것**
- 패딩과 스트라이드가 어떻게 적용되는지에 따라 밸리드 패딩, 풀 패딩, 세임 패딩이라고 부름
- **밸리드 패딩(valid padding)**
  - 앞에서 교차 상관을 싸이파이로 구현할 때, **`mode` 매개변수에 `valid`를 지정한 것**이 바로 밸리드 패딩을 적용한 예
  - **원본 배열에 패딩을 추가하지 않고**, 미끄러지는 배열이 원본 배열의 끝으로 갈 때까지 교차 상관을 수행
    - 따라서 밸리드 패딩의 결과로 얻는 배열의 크기는 원본 배열보다 항상 작음
  - **원본 배열의 각 원소가 합성곱 연산에 참여하는 정도가 서로 다름**
    - 원본 배열 양 끝 원소의 연산 참여도가 낮음 (책 p.238 그림 참고)
- **풀 패딩(full padding)**
  - **원본 배열의 모든 요소가 동일하게 연산에 참여하는 패딩 방식** 
  - 원본 배열의 원소가 연산에 동일하게 참여하려면, **원본 배열의 양 끝에 가상의 원소를 추가**해야 함
    - 가상의 원소로 **0을 사용**하기 때문에, 이를 **제로 패딩(zero padding)** 이라고 부름
  - 적절한 개수의 제로 패딩을 추가하면 원본 배열의 모든 원소가 연산에 동일하게 참여 가능 (책 p.239 그림 참고)
  - `correlate()` 함수에서 **매개변수 `mode`를 `full`로** 지정하여 풀 패딩 적용
    ```
    correlate(x, w, mode='full')

    # 출력
    # array([ 6, 34, 51, 48, 57, 24, 25, 16, 39, 29, 13, 10])
    ```
- **세임 패딩(same padding)**
  - **출력 배열의 길이가 원본 배열의 길이와 동일해지도록** 원본 배열에 제로 패딩 추가
  - `correlate()` 함수에서 **매개변수 `mode`를 `same`으로** 지정하여 세임 패딩 적용
    ```
    correlate(x, w, mode='same')

    # 출력
    # array([34, 51, 48, 57, 24, 25, 16, 39, 29])
    ```
- **합성곱 신경망에서는 대부분 세임 패딩을 사용**

<br>

#### 스트라이드(stride)
- **스트라이드 : 미끄러지는 배열의 간격을 조절하는 것**
- 지금까지 사용한 밸리드 패딩, 풀 패딩, 세임 패딩은 모두 1칸씩 미끄러지며 연산 수행
  - 스트라이드를 1로 지정하여 연산 수행된 것
- 스트라이드를 2로 지정하면, 2칸씩 미끄러지며 연산 수행
- **합성곱 신경망을 만들 때는 보통 스트라이드를 1로 지정**

<br>

#### 2차원 배열에서 합성곱 수행
- 지금까지는 이해를 위해 1차원 배열을 사용하여 합성곱, 패딩, 스트라이드를 알아보았지만, **합성곱 신경망은 대부분 2차원 배열에 대한 합성곱을 사용**
- 2차원 배열의 합성곱도 1차원 배열의 합성곱과 비슷하게 수행
- 합성곱의 수행 방향은
  - 원본 배열의 **왼쪽에서 오른쪽으로**,
  - **위에서 아래쪽으로 1칸씩 이동**하며 배열 원소끼리 곱하는 것
  - 책 p.241 그림 참고
    - 2차원 원본 배열 x, 미끄러지는 배열 w
    - 원본 배열의 왼쪽 모서리 끝에 미끄러지는 배열을 맞추고 합성곱 수행
    - 미끄러지는 배열을 오른쪽으로 1칸 옮겨서 합성곱 수행
    - 오른쪽 끝에 도달하면, 아래로 1칸 내리고 다시 왼쪽 끝부터 합성곱 수행
    - 그림의 원본 배열의 크기는 3*3이므로, 밸리드 패딩을 하면 미끄러지는 배열이 이동하는 횟수는 총 4번 
- **싸이파이의 `correlate2d()` 함수로 2차원 배열의 합성곱 계산**
  ```
  x = np.array([[1, 2, 3],
                [4, 5, 6],
                [7, 8, 9]])
  w = np.array([[2, 0], [0, 0]])

  from scipy.signal import correlate2d
  correlate2d(x, w, mode='valid')

  # 출력
  # array([[ 2,  4],
           [ 8, 10]])
  ```
- **2차원 배열에서의 세임 패딩**
  - **오른쪽과 아래쪽 모서리에 제로 패딩 추가**
  - 세임 패딩 적용 (책 p.242 그림 참고)
  ```
  correlate2d(x, w, mode='same')

  # 출력
  # array([[ 2,  4,  6],
           [ 8, 10, 12],
           [14, 16, 18]])
  ```
    - 원본 배열의 크기와 같은 출력 배열 생성
- **2차원 배열에서의 스트라이드**
  - 미끄러지는 방향은 그대로 유지하면서, 미끄러지는 **간격의 크기만 증가**

<br>

#### 텐서플로로 합성곱 수행
- 지금까지는 합성곱을 위해 싸이파이를 사용
- 텐서플로에서 합성곱을 위해 제공하는 함수를 사용하여 싸이파이와 동일한 결과를 출력하는지 비교
- 합성곱 신경망이 기준이므로 **원본 배열을 입력**, **미끄러지는 배열을 가중치**라고 부를 것
- **합성곱 신경망의 입력은 일반적으로 4차원 배열**
  - **텐서플로에서 2차원 합성곱을 수행하는 함수는 `conv2d()`로, 입력으로 4차원 배열을 기대함**
    - 입력 이미지의 높이와 너비 외에 더 많은 차원이 필요하기 때문
  - **4차원 입력 배열**의 구성 (책 p.243 참고)
    - 입력에 2개의 샘플이 포함되어 있으며
    - 각 샘플은 R, G, B로 구분되는 3개의 컬러 채널을 가짐
    - 그림의 입력을 4차원 배열로 표현하면 (2, 3, 3, 3)
      - **배치, 샘플의 높이, 샘플의 너비, 컬러 채널의 차원** 의미
  - 입력과 곱해지는 가중치도 4개의 차원으로 구성
    - 그림의 경우를 배열로 표현하면 (2, 2, 3, 3)
      - **가중치의 높이, 너비, 채널, 가중치의 개수** 의미
  - 입력과 가중치에 **세임 패딩**을 적용하여 합성곱 수행하면, **(입력의 배치, 입력의 높이, 입력의 너비, 가중치의 개수)** 가 됨
    - 책 p.244 그림 참고
    - 일반적인 합성곱의 입력과 가중치의 채널 수는 동일
    - 즉, 채널 방향으로는 가중치가 이동하지 않음
- **텐서플로를 활용하여 2차원 배열을 4차원 배열로 바꿔 합성곱 수행**
  - 넘파이의 **`reshape()`** 메서드로, 입력 x와 가중치 w를 2차원 배열에서 **4차원 배열로 변환**
  - 넘파이의 **`astype()`** 메서드로, 텐서플로는 실수형의 입력을 기대하므로 **입력의 자료형을 실수로 변환**
  - 배치와 컬러 채널은 1
   ```
   import tensorflow as tf
   x_4d = x.astype(np.float).reshape(1, 3, 3, 1)
   w_4d = w.reshape(2, 2, 1, 1)
   ```
   - 스트라이드는 1 적용
   - 패딩은 세임 패딩 적용
   - 텐서플로의 `conv2d()` 함수의 패딩 옵션은 대문자 사용
   ```
   c_out = tf.nn.conv2d(x_4d, w_4d, strides=1, padding='SAME')
   ```
   - **`conv2d()` 함수는 결과값으로 텐서플로의 `Tensor` 객체를 반환**
     - **텐서(tensor) : 텐서플로에서는 다차원 배열을 텐서라고 부름**
   - `Tensor` 객체의 `numpy()` 메서드를 사용하면 텐서를 넘파이 배열로 변환 가능
   - 배치 차원과 컬러 차원을 제거하고 편의상 (3, 3) 크기로 변환하여 출력
   ```
   c_out.numpy().reshape(3, 3)

   # 출력
   # array([[ 2.,  4.,  6.],
            [ 8., 10., 12.],
            [14., 16., 18.]])
   ```
   - 2차원 배열로 실습했던 `correlate2d()`의 출력 결과와 동일하지만, 실제 **텐서플로의 `conv2d()` 함수에 전달되는 매개변수의 값은 4차원 배열**임을 잊지 말기

<br>

#### 패션 MNIST 데이터 세트를 합성곱 신경망에 적용하면
- 07장에서는 패션 MNIST 데이터 세트를 `MultiClassNetwork` 클래스에 적용하며 28 * 28 크기의 입력을 일렬로 펼쳐서 사용
  - 따라서 가중치의 개수도 많이 필요했음
- **합성곱 신경망에서는 28 * 28 크기의 입력을** 펼치지 않고 **그대로 사용**
  - 따라서 3 * 3 또는 5 * 5 크기의 가중치로 합성곱 적용
  - 가중치 배열의 크기는 훨씬 작아졌고, 입력의 특징을 더 잘 찾기 때문에 **합성곱 신경망이 이미지 분류에서 뛰어난 성능 발휘 가능**

<br>

#### 합성곱의 가중치를 필터(filter) 또는 커널(kernel)이라고 부름
- 이 책에서는
  - 합성곱의 **필터 1개**를 지칭할 때는 **'커널'**
  - 합성곱의 **필터 전체**를 지칭할 때는 일반 신경망과 동일하게 **'가중치'**

<br>
<br>

### 8-2. 풀링 연산
- 합성곱 신경망에서는
  - **합성곱층 : 합성곱이 일어나는 층**
  - **풀링층 : 풀링이 일어나는 층**
  - **특성 맵(feature map) : 합성곱층과 풀링층에서 만들어진 결과**
- **합성곱층 뒤에 풀링층이 뒤따르는 형태**는 합성곱 신경망의 전형적인 모습
  - 입력이 합성곱층을 통과할 때, 합성곱과 활성화 함수가 적용되어 특성 맵이 만들어짐
  - 특성 맵이 풀링층을 통과하여 또 다른 특성 맵이 만들어짐
- **풀링(pooling) : 특성 맵을 스캔하며 최댓값을 고르거나 평균값을 계산하는 것**
- 합성곱 신경망에서는 최대 풀링과 평균 풀링을 주로 사용

<br>

#### 최대 풀링(max pooling)
- **최댓값을 고르는 풀링 방식**
- **풀링 영역의 크기는 보통 2 * 2**를 지정
- **스트라이드는** 일반적으로 **풀링의 한 모서리 크기**로 지정
  - 즉, 스트라이드는 2를 적용
  - 풀링 영역이 겹쳐지지 않도록 스캔
  - (책 p.247 그림 참고)
- **2 * 2 풀링은 특성 맵의 크기를 절반으로 줄임** (면적은 1/4)
  - **특성 맵의 한 요소가 입력의 더 넓은 영역을 바라볼 수 있는 효과**
  - 합성곱층에서 스트라이드를 크게 지정하여 특성 맵의 크기를 줄이면 안되나?
    - 경험적으로 합성곱층에 세임 패딩을 적용하고, 풀링층에서 특성 맵의 크기를 줄이는 것이 더 효과적

<br>

#### 평균 풀링(average pooling)
- **풀링 영역의 평균값을 계산하는 방식**
  - (책 p.248 그림 참고)
- **연구자들은** 보통 평균 풀링보다 **최대 풀링을 선호함**
  - 평균 풀링은 합성곱층을 통과하는 특징들을 희석시킬 가능성이 높기 때문
    - 입력에서 합성곱 필터가 찾고자 하는 부분은, 특성 맵의 가장 큰 값으로 활성화되는데, 평균 풀링은 가장 큰 특성의 값을 상쇄시킴
  - **최대 풀링은 가장 큰 특징을 유지시키는 성질**
    - 이미지 분류 작업에 적합

<br>

#### 최대 풀링과 평균 풀링 수행
- **텐서플로의 `max_pool2d()` 함수** 사용
  - 매개변수 값으로 풀링 크기와 스트라이드만 전달하면, 자동으로 **최대 풀링 수행**
  ```
  x = np.array([[1, 2, 3, 4],
                [5, 6, 7, 8],
                [9, 10, 11, 12],
                [13, 14, 15, 16]])
  x = x.reshape(1, 4, 4, 1)
  ```
  - 1-16의 값이 들어있는 4 * 4 크기의 배열 생성
  - 1 * 4 * 4 * 1 크기의 배열로 변형
    - `max_pool2d()` 함수는 `conv2d()` 함수와 마찬가지로, 첫 번째 차원으로 배치 차원을 기대하기 때문
  ```
  p_out = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')
  p_out.numpy().reshape(2, 2)

  # 출력
  # array([[ 6.,  8.],
           [14., 16.]])
  ```
  - **`ksize`** 매개변수 : 풀링의 크기 지정
  - **`strides`** 매개변수 : 스트라이드 크기 지정
  - `max_pool2d()` 함수가 반환한 `Tensor` 객체를 `numpy()` 메서드로 변환 후, 2 * 2 크기의 2차원 배열로 변형
  - 출력 결과를 통해 **성공적으로 최대 풀링**이 된 것 확인 가능
- 기억할 것
  - **풀링층에는 학습되는 가중치가 없음**
  - 풀링은 배치 차원이나 채널 차원으로 적용되지 않음
    - **풀링층을 통과하기 전후로 배치 크기와 채널 크기는 동일**
    - 풀링은 각 샘플마다, 각 채널마다 독립적으로 수행

<br>
<br>

### 8-3. 합성곱 신경망의 구조

#### 렐루(ReLU) 함수
- 이전까지는
  - 은닉층은 시그모이드 함수를 활성화 함수로 사용
  - 출력층은 
    - 이진 분류일 경우, 시그모이드 함수 사용
    - 다중 분류일 경우, 소프트맥스 함수 사용
- **렐루 함수는 주로 합성곱층에 적용되는 활성화 함수**로, 합성곱 신경망의 성능을 더 높여줌
- **0보다 큰 값은 그대로 통과시키고, 0보다 작은 값은 0으로 만듦** (책 p.250 그래프, 수식 참고)
  - x < 0 경우 : y는 0
  - x > 0 경우 : y는 x의 값 그대로

<br>

#### 렐루 함수 구현
- **넘파이의 `maximum()` 함수**를 사용하여 간단하게 구현 가능
  - 함수는 두 매개변수 값 중 큰 값을 골라 반환
  ```
  def relu(x):
      return np.maximum(x, 0)
  ```
  - 샘플 데이터를 렐루 함수에 넣어 확인
  - `relu()` 함수의 반환값을 보면, 음수가 모두 사라지고 양수만 반환됨
  ```
  x = np.array([-1, 2, -3, 4, -5])
  relu(x)

  # 출력
  # array([0, 2, 0, 4, 0])
  ```
  - **텐서플로가 제공하는 렐루 함수는 `relu()`**
  - **`Tensor` 객체를 반환**하므로, 화면에 출력하려면 넘파이로 변환 필요
  ```
  r_out = tf.nn.relu(x)
  r_out.numpy()

  # 출력
  # array([0, 2, 0, 4, 0])
  ```

<br>

#### 렐루 함수의 도함수
- **입력이 0보다 크면 1** (y와 x의 값이 같으므로)
- **입력이 0보다 작으면 0** (항상 y=0이므로)
- x=0일 때 그래프가 크게 꺾이므로(연속적이지 않으므로) 기울기가 정의되지 않음
- 하지만 대부분의 딥러닝 패키지들은 x=0인 경우 도함수를 0으로 생각

<br>

#### 합성곱 신경망에서 일어나는 일들과 구조
- **합성곱 신경망에 주입될 입력 데이터에는 채널이 있음**
  - 합성곱 신경망은 이미지의 2차원 형태를 입력으로 그대로 사용하므로, 이미지를 한 줄로 펼칠 필요가 없음
    - **-> 이미지 정보가 손상되지 않는다는 장점**
  - **고려할 점은 이미지는 채널이라는 차원을 하나 더 가진다는 것**
  - **채널(channel)** : 이미지의 픽셀이 가진 색상을 표현하기 위해 필요한 정보
    - 색상은 빨간색, 파란색, 초록색의 조합으로 표현 가능하며, 이를 RGB라고 부름
    - 흑백 이미지의 경우, 흑백의 강도만 표현하면 되므로 하나의 채널만 가짐
  - 이러한 입력 데이터에서 합성곱과 풀링이 어떻게 적용되는지 알아보자
- **합성곱층에서 일어나는 일**
  - **이미지의 모든 채널에 합성곱이 한 번에 적용되어야 하므로, 커널의 마지막 차원은 입력 채널의 개수와 동일해야 함**
    - 3 * 3 크기의 커널을 사용한다면, 4 * 4 * 3  구조의 이미지의 경우, 커널 배열의 크기는 3 * 3 * 3이 되어야 함
    - 이미지가 4 * 4 * 10의 구조를 가지고 있다면(10개의 채널을 가진 이미지), 커널 배열의 크기도 3 * 3 * 10으로 마지막 차원의 개수를 동일하게 맞춰야 함
  - 이미지와 커널이 준비되면 **합성곱 수행** (책 p.253 그림 참고)
  - 4 * 4 * 3으로 구성된 이미지 위를 3 * 3 * 3 크기의 커널이 이동하며 합성곱을 4번 수행하므로, 2 * 2 크기의 특성 맵이 만들어짐
    - 입력 채널은 커널의 채널과 각각 합성곱을 수행
    - 그 후 합성곱의 전체 결과를 더하여 특성 맵 1조각을 만듦
    - 책의 그림에서는 커널이 하나이므로 이미지의 특징 하나만 감지
    - 만약 이미지에서 여러 개의 특징을 감지하려면 복수 개의 커널을 사용해야 함
  - 합성곱이 완료되면, **다음 층에서 사용하게 될 특성 맵**이 만들어짐
- **풀링층에서 일어나는 일**
  - 합성곱층을 통해 만들어진 특성 맵에 **활성화 함수로 렐루 함수를 적용한 후, 풀링**을 적용함 (책 p.254 그림 참고)
  - 풀링은 특성 맵의 크기를 줄여주므로, 특성 맵의 크기가 2 * 2 * 5일 때 2 * 2 풀링이 적용되면 1 * 1 * 5 크기의 특성 맵이 만들어짐
    - **특성 맵의 크기가 절반으로 줄어드는 것**
    - **채널의 크기는 줄어들지 않음**
  - 합성곱층과 풀링층은 한 신경망 안에 여러 개가 들어 있을 수도 있음

<br>

#### 특성 맵을 펼쳐 완전 연결 신경망에 주입
- 합성곱층과 풀링층을 통과하여 얻은 **특성 맵은, 일렬로 펼쳐 완전 연결층에 입력으로 주입** (책 p.254 그림 참고)
  - 이러한 완전 연결층은 한 신경망 안에 여러 개가 들어 있을 수도 있음
- 완전 연결층의 출력은 출력층의 뉴런과 연결됨
- 이 층들은 합성곱층이 찾는 특성을 사용하여 최종 분류 단계를 수행하는 과정으로 볼 수 있음
  - 풀링층을 통과한 1 * 1 * 5 크기의 특성 맵을 일렬로 펼쳐 완전 연결층에 주입
  - 그다음 출력층과 다중 분류를 위한 소프트맥스 함수를 통과하여 최종 출력을 만듦

<br>

#### 정리하자면
- 합성곱층에 주입된 이미지는 특성을 감지하는 커널에 의해 특성 맵으로 만들어진다.
- 합성곱층을 통해 만들어진 특성 맵은 활성화 함수와 풀링층을 거져 더 작은 특성 맵이 된다. 이때 특성 맵의 채널 개수는 그대로 유지된다.
- 풀링층을 통해 만들어진 특성 맵을 일렬로 펼쳐 완전 연결층에 주입한다. 그다음 출력층을 통과하여 최종 예측을 만든다.

<br>
<br>

### 8-4. 합성곱 신경망을 만들고 훈련하기
- 지금까지는 순수 파이썬으로 신경망을 구현해 왔지만, 합성곱 신경망은 그렇게 하기 어려움
- 따라서 텐서플로가 제공하는 합성곱 함수와 자동 미분 기능을 사용하여 합성곱 신경망 구현할 것

<br>

#### 구현하게 될 합성곱 신경망의 전체 구조
- 책 p.256 그림 참고
- 28 * 28 크기의 흑백 이미지와 3 * 3 크기의 커널 10개로 합성곱 수행
- 그다음 2 * 2 크기의 최대 풀링을 수행하여 14 * 14 * 10로 특성 맵의 크기를 줄일 것
- 이 특성 맵을 일렬로 펼쳐서 100개의 뉴런을 가진 완전 연결층과 연결시킬 것
- 그런 다음, 10개의 클래스를 구분하기 위한 소프트맥스 함수를 연결(의류 종류가 총 10개였음)

<br>

#### 합성곱 신경망의 정방향 계산 구현
- 구현할 **합성곱 신경망 클래스는 `ConvolutionNetwork`**
- 이전에 구현한 클래스들과 코드 구성은 대체로 비슷하고, 합성곱, 렐루 함수, 풀링이 적용된 것만 다름
  - 먼저 합성곱과 풀링이 일어나는 정방향 계산 부분을 담당하는 `forpass()` 메서드 구현
    - **`MultiClassNetwork` 클래스의 `forpass()` 메서드에 있던 z1, a1, z2를 계산하는 식은 그대로** 두고, 그 앞에 합성곱과 풀링층을 추가하여 코드 작성
1. **합성곱 적용하기**
   - `conv2d()` 함수를 통해 합성곱을 수행한 다음, 절편 `self.conv_b`를 더해야 함
     - 절편은 커널마다 1개씩 필요하므로 총 10개의 절편이 필요
     - `conv2d()` 함수의 결과로 만들어지는 특성 맵의 크기는 28 * 28 * 10
     - 크기가 10인 1차원 배열 `self.conv_b`는 자동으로 `conv2d()` 함수의 결과 마지막 차원에 브로드캐스팅됨
   ```
   def forpass(self, x):
       # 3x3 합성곱 연산 수행
       c_out = tf.nn.conv2d(x, self.conv_w, strides=1, padding='SAME') + self.conv_b
   ```
   - 합성곱을 수행하는 `conv2d()` 함수에 전달한 매개변수 값
     - **`self.conv_w` : 합성곱에 사용할 가중치**
       - 3 * 3 * 1 크기의 커널을 10개 사용하므로, 가중치의 전체 크기는 3 * 3 * 1 * 10  
     - `stride`, `padding` : 특성 맵의 가로와 세로 크기를 일정하게 만들기 위하여 각각 1, 'SAME'으로 지정
2. **렐루 함수 적용하기**
   - 합성곱 계산을 수행한 다음에 렐루 함수를 적용하여 합성곱층 완성
   ```
   def forpass(self, x):
       ...
       # 렐루 함수 적용
       r_out = tf.nn.relu(c_out)
       ...
   ```
3. **풀링 적용하고 완전 연결층 수정하기**
   - `max_pool2d()` 함수를 사용하여 2 * 2 크기의 풀링 적용
     - 이 단계에서 만들어진 특성 맵의 크기는 14 * 14 * 10
   - 풀링으로 특성 맵의 크기를 줄인 다음, `tf.reshape()` 함수를 사용해 일렬로 펼침
     - 이때 배치 차원을 제외한 나머지 차원만 펼쳐야 함
   - 그 다음 코드는 완전 연결층에 해당
   - 여기서는 **`np.dot()` 함수를 텐서플로의 `tf.matmul()` 함수로 변경**
     - `conv2d()`와 `max_pool2d()` 등이 `Tensor` 객체를 반환하기 때문
   - 완전 연결층의 활성화 함수도 시그모이드 함수 대신 **렐루 함수를 사용**
   ```
    def forpass(self, x):
        ...
        # 2x2 최대 풀링 적용
        p_out = tf.nn.max_pool2d(r_out, ksize=2, strides=2, padding='VALID')
        # 첫 번째 배치 차원을 제외하고 출력을 일렬로 펼침
        f_out = tf.reshape(p_out, [x.shape[0], -1])
        z1 = tf.matmul(f_out, self.w1) + self.b1     # 첫 번째 층의 선형 식 계산
        a1 = tf.nn.relu(z1)                          # 활성화 함수 적용
        z2 = tf.matmul(a1, self.w2) + self.b2        # 두 번째 층의 선형 식 계산
        return z2
   ```

<br>

#### 합성곱 신경망의 역방향 계산 구현
- 합성곱의 역방향 계산을 직접 구현하는 것은 복잡할 뿐 아니라, 학습에 유용하지도 않음
- 여기에서는 그레이디언트를 구하기 위해, 텐서플로의 자동 미분 기능을 사용할 것
- **자동 미분(automatic differentiation)의 사용 방법**
  - 텐서플로와 같은 딥러닝 패키지들은 사용자가 작성한 연산을 계산 그래프로 만들어 자동 미분 기능을 구현함
  - **자동 미분 기능을 사용하면 임의의 파이썬 코드나 함수에 대한 미분값을 계산할 수 있음**
  - ex) 사용자가 작성한 연산을 바탕으로 자동 미분한 예시
    ```
    x = tf.Variable(np.array([1.0, 2.0, 3.0]))
    with tf.GradientTape() as tape:
        y = x ** 3 + 2 * x + 5
    
    # 그레이디언트 계산
    print(tape.gradient(y, x))

    # 출력
    # tf.Tensor([ 5. 14. 29.], shape=(3,), dtype=float64)
    ```
  - 텐서플로의 자동 미분 기능을 사용하려면 **`with` 블럭으로 `tf.GradientTape()` 객체가 감시할 코드를 감싸야 함**
    - **`tape` 객체**는 `with` 블럭 안에서 일어나는 모든 연산을 기록하고, 텐서플로 변수인 `tf.Variable` 객체를 자동으로 추적
  - **그레이디언트를 계산하려면** 미분 대상 객체와 변수를 **`tape` 객체의 `gradient()` 메서드**에 전달해야 함
  - 위 예제에서 수행하는 계산은 간단한 3차 방정식
1. **역방향 계산 구현하기**
   - `MultiClassNetwork` 클래스에서는 `training()` 메서드에서 `backprop()` 메서드를 호출하여 가중치 업데이트
   - 자동 미분 기능을 사용하면 `ConvolutionNetwork`의 `backprop()` 메서드를 구현할 필요가 없음
   - 따라서 `training()` 구성도 간단해짐
   ```
   def training(self, x, y):
       m = len(x)                    # 샘플 개수 저장
       with tf.GradientTape() as tape:
           z = self.forpass(x)       # 정방향 계산 수행
           # 손실 계산
           loss = tf.nn.softmax_cross_entropy_with_logits(y, z) 
           loss = tf.reduce_mean(loss)
       ...
   ```
   - `tf.nn.softmax_cross_entropy_with_logits()`
     - 정방향 계산의 결과(z)와 타깃(y)을 기반으로 손실값 계산
     - **크로스 엔트로피 손실**과 **그레이디언트 계산**을 올바르게 처리해 주므로 편리함
     - 배치의 각 샘플에 대한 손실을 반환하므로 `reduce_mean()` 함수로 평균 계산
2. **그레이디언트 계산하기**
   - 이제 가중치와 절편을 업데이트해야 함
   - `tape.gradient()` 사용하면 그레이디언트 자동으로 계산
   - 합성곱층의 가중치와 절편인 `con_w`와 `con_b`를 포함하여 **그레이디언트가 필요한 가중치를 리스트로 나열**
   - `optimizer.apply_gradients()`
     - 텐서플로는 여러 종류의 경사 하강법 알고리즘을 클래스로 미리 구현해 놓음
       - 경사 하강법 알고리즘을 바꾸어 가며 테스트할 때 가중치를 업데이트하는 코드를 일일이 고쳐야 한다면 아주 번거로울 것
       - **텐서플로의 옵티마이저를 사용하면 간단하게 알고리즘을 바꾸어 테스트 가능**
     - `apply_gradients()` 메서드에는 **그레이디언트와 가중치를 튜플로 묶은 리스트를 전달해야 함**
     - 여기에서는 파이썬의 `zip` 반복자를 사용하여 이를 구현
   ```
   def training(self, x, y):
   ...
       weights_list = [self.conv_w, self.conv_b,
                        self.w1, self.b1, self.w2, self.b2]
       # 가중치에 대한 그래디언트 계산
       grads = tape.gradient(loss, weights_list)
       # 가중치 업데이트
       self.optimizer.apply_gradients(zip(grads, weights_list))
   ```

<br> 

#### 옵티마이저 객체를 만들어 가중치 초기화
- `training()` 메서드에 등장하는 `self.optimizer()`를 `fit()` 메서드에서 만들어 보기
- 여기서는 확률적 경사 하강법(SGD) 사용
1. **`fit()` 메서드 수정하기**
   - 옵티마이저 객체 생성 부분만 제외하면 `MultiClassNetwork` 클래스의 `fit()` 과 거의 동일
   - 텐서플로는 `tf.optimizers` 모듈 아래에 여러 종류의 경사 하강법을 구현해 놓음
     - **SGD 옵티마이저 객체는 기본 경사 하강법**
   - SGD 옵티마이저 생성 코드를 추가한 코드
   ```
   def fit(self, x, y, epochs=100, x_val=None, y_val=None):
        self.init_weights(x.shape, y.shape[1])    # 은닉층과 출력층의 가중치 초기화
        self.optimizer = tf.optimizers.SGD(learning_rate=self.lr)
        # epochs만큼 반복
        for i in range(epochs):
            print('에포크', i, end=' ')
            # 제너레이터 함수에서 반환한 미니배치 순환
            batch_losses = []
            for x_batch, y_batch in self.gen_batch(x, y):
                print('.', end='')
                self.training(x_batch, y_batch)
                # 배치 손실 기록
                batch_losses.append(self.get_loss(x_batch, y_batch))
            print()
            # 배치 손실 평균 내어 훈련 손실 값으로 저장
            self.losses.append(np.mean(batch_losses))
            # 검증 세트에 대한 손실 계산
            self.val_losses.append(self.get_loss(x_val, y_val))
   ```
2. **`init_weights()` 메서드 수정하기**
   - 가중치를 초기화하는 `init_weights()` 메서드는 큰 변화가 있음
     - **가중치를 `glorot_uniform()` 함수로 초기화한다는 점**
     - 텐서플로의 자동 미분 기능을 사용하기 위해 **가중치를 `tf.Variable()` 함수로 만들어야 한다는 점**
   - 합성곱의 가중치와 완전 연결층의 가중치를 `tf.Variable()` 함수로 선언할 때, 입력값에 따라 자료형이 자동으로 결정됨
   - `np.zeros()` 함수는 기본적으로 64비트 실수를 생성
     - 따라서 절편 변수를 가중치 변수와 동일하게 32비트 실수로 맞추기 위해 `dtype` 매개변수에 `float` 지정
   ```
   def init_weights(self, input_shape, n_classes):
        g = tf.initializers.glorot_uniform()
        self.conv_w = tf.Variable(g((3, 3, 1, self.n_kernels)))
        self.conv_b = tf.Variable(np.zeros(self.n_kernels), dtype=float)
        n_features = 14 * 14 * self.n_kernels
        self.w1 = tf.Variable(g((n_features, self.units)))          # (특성 개수, 은닉층의 크기)
        self.b1 = tf.Variable(np.zeros(self.units), dtype=float)    # 은닉층의 크기
        self.w2 = tf.Variable(g((self.units, n_classes)))           # (은닉층의 크기, 클래스 개수)
        self.b2 = tf.Variable(np.zeros(n_classes), dtype=float)     # 클래스 개수
   ```

<br>

#### `glorot_uniform()`
- `glorot_uniform()` 함수 : 가중치를 초기화할 때 **글로럿(Glorot) 초기화라는 방법을 사용**할 수 있게 해 줌
- 지금까지는 넘파이로 난수를 만들어 가중치를 초기화했음
- 신경망 모델이 너무 커지면 손실 함수도 복잡해지기 때문에 **출발점에 따라 결과가 달라질 수 있음**
  - 책 p.263 그래프 참고
  - 경사 하강법은 출발점으로부터 기울기가 0이 되는 최저점을 찾아감
  - **지역 최적점(local minimum)** : 가중치를 적절하게 초기화하지 않으면 출발점이 적절하지 않은 곳에 설정되므로, **엉뚱한 곳**에서 최적점이라는 판단을 내릴 수 있는데, 이렇게 찾은 지점을 의미함
  - **전역 최적점(global minimum)** : 가중치를 적절하게 초기화했다면 출발점이 적절한 곳에 설정되므로 **올바른 최적점**을 찾게 되는데, 이렇게 찾은 지점을 의미함
- **글로럿 초기화 방식으로 가중치 초기화**
- **`glorot_uniform()` 함수는 일정 구간 사이에서 균등하게 난수를 발생시켜 가중치 초기화** (책 p.263 수식 참고)
  - 텐서플로의 케라스는 기본값으로 글로럿 초기화를 사용했음
- 글로럿 초기화를 사용하는 방법은 간단함
  - **`glorot_uniform()` 함수에서 만든 객체를 호출할 때, 필요한 가중치 크기를 전달하면 됨**
  - `init_weights()`에서 `conv_w`, `w1`, `w2`의 3개 변수를 글로럿 방식으로 초기화함
    - `conv_w` : 합성곱 영역의 너비와 높이는 3 * 3이고, 흑백 이미지의 입력 채널은 하나이므로, 커널의 크기는 3 * 3 * 1
      - 이런 합성곱 커널을 `n_kernels`만큼 만들기 위해 3 * 3 * 1 * n_kernels 크기의 4차원 배열로 초기화
    - 가중치 w1의 크기는 14 * 14 * n_kernels
      - 합성곱과 풀링층을 거치면 입력 이미지의 높이와 너비가 28에서 14로 줄어듬
      - 만들어진 특성 맵의 개수는 n_kernels
      - 이 배열이 일렬로 펼쳐져서 완전 연결층에 주입됨

<br>

- 전체 코드
```
import tensorflow as tf

class ConvolutionNetwork:
    
    def __init__(self, n_kernels=10, units=10, batch_size=32, learning_rate=0.1):
        self.n_kernels = n_kernels  # 합성곱의 커널 개수
        self.kernel_size = 3        # 커널 크기
        self.optimizer = None       # 옵티마이저
        self.conv_w = None          # 합성곱 층의 가중치
        self.conv_b = None          # 합성곱 층의 절편
        self.units = units          # 은닉층의 뉴런 개수
        self.batch_size = batch_size  # 배치 크기
        self.w1 = None              # 은닉층의 가중치
        self.b1 = None              # 은닉층의 절편
        self.w2 = None              # 출력층의 가중치
        self.b2 = None              # 출력층의 절편
        self.a1 = None              # 은닉층의 활성화 출력
        self.losses = []            # 훈련 손실
        self.val_losses = []        # 검증 손실
        self.lr = learning_rate     # 학습률

    def forpass(self, x):
        # 3x3 합성곱 연산 수행
        c_out = tf.nn.conv2d(x, self.conv_w, strides=1, padding='SAME') + self.conv_b
        # 렐루 활성화 함수 적용
        r_out = tf.nn.relu(c_out)
        # 2x2 최대 풀링 적용
        p_out = tf.nn.max_pool2d(r_out, ksize=2, strides=2, padding='VALID')
        # 첫 번째 배치 차원을 제외하고 출력을 일렬로 펼침
        f_out = tf.reshape(p_out, [x.shape[0], -1])
        z1 = tf.matmul(f_out, self.w1) + self.b1     # 첫 번째 층의 선형 식 계산
        a1 = tf.nn.relu(z1)                          # 활성화 함수 적용
        z2 = tf.matmul(a1, self.w2) + self.b2        # 두 번째 층의 선형 식 계산
        return z2
    
    def init_weights(self, input_shape, n_classes):
        g = tf.initializers.glorot_uniform()
        self.conv_w = tf.Variable(g((3, 3, 1, self.n_kernels)))
        self.conv_b = tf.Variable(np.zeros(self.n_kernels), dtype=float)
        n_features = 14 * 14 * self.n_kernels
        self.w1 = tf.Variable(g((n_features, self.units)))          # (특성 개수, 은닉층의 크기)
        self.b1 = tf.Variable(np.zeros(self.units), dtype=float)    # 은닉층의 크기
        self.w2 = tf.Variable(g((self.units, n_classes)))           # (은닉층의 크기, 클래스 개수)
        self.b2 = tf.Variable(np.zeros(n_classes), dtype=float)     # 클래스 개수
        
    def fit(self, x, y, epochs=100, x_val=None, y_val=None):
        self.init_weights(x.shape, y.shape[1])    # 은닉층과 출력층의 가중치 초기화
        self.optimizer = tf.optimizers.SGD(learning_rate=self.lr)
        # epochs만큼 반복
        for i in range(epochs):
            print('에포크', i, end=' ')
            # 제너레이터 함수에서 반환한 미니배치 순환
            batch_losses = []
            for x_batch, y_batch in self.gen_batch(x, y):
                print('.', end='')
                self.training(x_batch, y_batch)
                # 배치 손실 기록
                batch_losses.append(self.get_loss(x_batch, y_batch))
            print()
            # 배치 손실 평균내어 훈련 손실 값으로 저장
            self.losses.append(np.mean(batch_losses))
            # 검증 세트에 대한 손실 계산
            self.val_losses.append(self.get_loss(x_val, y_val))

    # 미니배치 제너레이터 함수
    def gen_batch(self, x, y):
        bins = len(x) // self.batch_size                   # 미니배치 횟수
        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞기
        x = x[indexes]
        y = y[indexes]
        for i in range(bins):
            start = self.batch_size * i
            end = self.batch_size * (i + 1)
            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환
            
    def training(self, x, y):
        m = len(x)                    # 샘플 개수 저장
        with tf.GradientTape() as tape:
            z = self.forpass(x)       # 정방향 계산 수행
            # 손실 계산
            loss = tf.nn.softmax_cross_entropy_with_logits(y, z)
            loss = tf.reduce_mean(loss)

        weights_list = [self.conv_w, self.conv_b,
                        self.w1, self.b1, self.w2, self.b2]
        # 가중치에 대한 그래디언트 계산
        grads = tape.gradient(loss, weights_list)
        # 가중치 업데이트
        self.optimizer.apply_gradients(zip(grads, weights_list))
   
    def predict(self, x):
        z = self.forpass(x)                 # 정방향 계산 수행
        return np.argmax(z.numpy(), axis=1) # Tensor 객체를 넘파이 배열로 바꿈, 가장 큰 값의 인덱스 반환
    
    def score(self, x, y):
        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환
        return np.mean(self.predict(x) == np.argmax(y, axis=1))

    def get_loss(self, x, y):
        z = self.forpass(x)                 # 정방향 계산 수행
        # 손실을 계산하여 저장
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, z))
        return loss.numpy()
```

<br>

#### 합성곱 신경망 훈련
1. **데이터 세트 불러오기**
   - 텐서플로를 사용해 패션 MNIST 데이터 세트 불러옴
   ```
   (x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
   ```
2. **사이킷런을 사용해 훈련 데이터 세트를 훈련 세트와 검증 세트로 나누기**
   ```
   from sklearn.model_selection import train_test_split
   x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)
   ```
3. **타깃을 원-핫 인코딩으로 변환하기**
   - `y_train`, `y_val`은 정수로 이루어진 1차원 배열
   - 합성곱 신경망의 타깃으로 사용하려면 두 배열의 요소들을 원-핫 인코딩으로 변경 필요
   ```
   y_train_encoded = tf.keras.utils.to_categorical(y_train)
   y_val_encoded = tf.keras.utils.to_categorical(y_val)
   ```
4. **입력 데이터 준비하기**
   - 합성곱 신경망은 입력 데이터(이미지)를 일렬로 펼칠 필요가 없음
   - **높이와 너비 차원을 그대로 유지한 채 신경망에 주입**
     - 대신 **마지막에 컬러 채널을 추가**해야 함
     - 사실 흑백 이미지에는 컬러 채널이 없지만, 명암을 나타내는 1차원 채널이 있다고 가정
   - 넘파이 `reshape()` 메서드를 사용하여 마지막 차원 간단히 추가
   ```
   x_train = x_train.reshape(-1, 28, 28, 1)
   x_val = x_val.reshape(-1, 28, 28, 1)
   ```
   - 준비된 입력 데이터를 확인해 보면 마지막 차원이 추가된 것 알 수 있음
   ```
   x_train.shape

   # 출력
   # (48000, 28, 28, 1)
   ```
5. **입력 데이터 표준화 전처리하기**
   - 입력 데이터는 이미지이므로 0-255 사이의 정수로 픽셀 강도 표현
   - 경사 하강법을 사용하므로 07장에서와 같이 입력 데이터를 255로 나누어 0-1 사이의 값으로 조정
   ```
   x_train = x_train / 255
   x_val = x_val / 255
   ```
6. **모델 훈련하기**
   - `ConvolutionNetwork` 클래스 객체를 생성하고, `fit()` 메서드를 호출하여 모델 훈련
   - 합성곱 커널은 10개 사용, 완전 연결층의 뉴런은 100개 사용
   - 배치 크기는 128개, 학습률은 0.01로 지정
   - 이 모델을 20번의 에포크 동안 훈련
   ```
   cn = ConvolutionNetwork(n_kernels=10, units=100, batch_size=128, learning_rate=0.01)
   cn.fit(x_train, y_train_encoded, x_val=x_val, y_val=y_val_encoded, epochs=20)
   ```
7. **훈련, 검증 손실 그래프 그리고 검증 세트의 정확도 확인하기**
   ```
   import matplotlib.pyplot as plt
   
   plt.plot(cn.losses)
   plt.plot(cn.val_losses)
   plt.ylabel('loss')
   plt.xlabel('iteration')
   plt.legend(['train_loss', 'val_loss'])
   plt.show()
   ```
   - 에포크가 진행되면서 훈련 손실과 검증 손실이 조금씩 차이가 나긴 하지만, **전반적으로 훈련이 잘 되고 있음**
   - 이 모델의 검증 세트에 대한 **정확도를 측정하면 88%** 에 가까움
   - 간단한 합성곱층을 이용하여 07장보다 조금 더 성능을 향상시킴
   ```
   cn.score(x_val, y_val_encoded)

   # 출력
   # 0.8806666666666667
   ```
   - 텐서플로의 자동 미분 기능과 합성곱 연산 함수를 사용하여 합성곱 신경망을 직접 만들어봄
   - 실전에서는 이렇게 저수준 연산을 사용하는 것보다, **케라스와 같은 고수준 API를 사용하는 것이 편리하고 생산적**

<br>
<br>

### 8-5. 케라스로 합성곱 신경망 만들기
- 케라스의 합성곱층은 `Conv2D` 클래스
- 최대 풀링은 `MaxPooling2D` 클래스 사용
- 특성 맵을 일렬로 펼칠 때는 `Flatten` 클래스 사용
1. **필요한 클래스들을 임포트하기**
   ```
   from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
   ```
2. **합성곱층 쌓기**
   - `Conv2D` 클래스의 
   - 첫 번째 매개변수 : 합성곱 커널의 개수
   - 두 번째 매개변수 : 합성곱 커널의 크기, 높이와 너비를 튜플로 전달
     - 합성곱 커널로는 전형적으로 3 * 3 또는 5 * 5 크기를 많이 사용
   - `activation` 매개변수에 렐루 활성화 함수 지정
   - 패딩은 세임 패딩 사용
     - `tf.nn.con2d()` 함수와 달리, 패딩 매개변수는 대소문자를 구분하지 않음
   - 07장에서 보았던 것처럼 `Sequential` 클래스에 층을 처음 추가할 때는 배치 차원을 제외한 입력의 크기를 지정
       - 여기에서는 패션 MNIST 이미지의 높이와 너비, 컬러 채널이 입력의 크기에 해당  
   ```
   conv1 = tf.keras.Sequential()
   conv1.add(Conv2D(10, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))
   ```
3. **풀링층 쌓기**
   - `MaxPooling2D` 클래스의
     - 첫 번째 매개변수 : 풀링의 높이와 너비를 나타내는 튜플
     - `strides` 매개변수 : 스트라이드 지정, 기본값은 풀링의 크기
     - `padding` 매개변수 : 패딩 지정, 기본값은 `valid`
   ```
   conv1.add(MaxPooling2D((2, 2)))
   ```
4. **완전 연결층에 주입할 수 있도록 특성 맵 펼치기**
   ```
   conv1.add(Flatten())
   ```
5. **완전 연결층 쌓기**
   - 첫 번째 완전 연결층 : 100개의 뉴런 사용, 렐루 활성화 함수 적용
   - 마지막 출력층 : 10개의 클래스에 대응하는 10개의 뉴런 사용, 소프트맥스 활성화 함수 적용
   ```
   conv1.add(Dense(100, activation='relu'))
   conv1.add(Dense(10, activation='softmax'))
   ```
6. **모델 구조 살펴보기**
   - 케라스를 사용해 합성곱 신경망을 아주 간단하게 만듦
   - **모델의 `summary()`** 메서드 : **모델의 구조를 자세히** 조사 가능
   ```
   conv1.summary()
   ```
   - (책 p.271 출력 결과와 설명 참고)

<br>

#### 합성곱 신경망 모델 훈련
1. **모델을 컴파일한 다음 훈련해 보기**
   - 다중 분류를 위한 크로스 엔트로피 손실 함수 사용
   - 정확도 관찰을 위해 `metrics` 매개변수에 `accuracy`를 리스트로 전달
   ```
   conv1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
   ```
2. **아담 옵티마이저 사용하기**
   - 지금까지는 최적화 알고리즘으로 기본 경사 하강법만 사용했음
   - 이번에는 **적응적 학습률 알고리즘 중 하나인 아담(Adam) 옵티마이저 사용**
   - 아담은 **손실 함수의 값이 최적값에 가까워질수록 학습률을 낮춰, 손실 함수의 값이 안정적으로 수렴될 수 있게** 해 줌
   - 20번의 에포크 동안 훈련
   ```
   history = conv1.fit(x_train, y_train_encoded, epochs=20, validation_data=(x_val, y_val_encoded))
   ```
3. **에포크에 따른 손실 그래프와 정확도 그래프 확인하기**
   ```
   plt.plot(history.history['loss'])
   plt.plot(history.history['val_loss'])
   plt.ylabel('loss')
   plt.xlabel('epoch')
   plt.legend(['train_loss', 'val_loss'])
   plt.show()
   
   plt.plot(history.history['accuracy'])
   plt.plot(history.history['val_accuracy'])
   plt.ylabel('accuracy')
   plt.xlabel('epoch')
   plt.legend(['train_accuracy', 'val_accuracy'])
   plt.show()
   ```
   - 지금까지 합성곱 신경망을 케라스로 정의하고 모델을 훈련해 봄
   - 마지막 에포크에서 검증 세트에 대한 정확도가 약 92%로 크게 증가함
   - 하지만 정확도와 손실을 그래프로 그려보니 **몇 번의 에포크 만에 검증 손실이 크게 증가**
     - **-> 과대적합이 일찍 발생했음을 의미**

<br>

#### 드롭아웃
- **신경망에서 과대적합을 줄이는 방법 중 하나**
- 드롭아웃은 **무작위로 일부 뉴런을 비활성화시킴**
  - -> **특정 뉴런에 과도하게 의존하여 훈련하는 것을 막아줌**
- 축구 경기에 출전하는 선수를 선발하는 것에 비유 (책 p.274 그림 참고)
  - **축구 선수를 출전 목록에서 무작위로 제외하는 것이 드롭아웃**
- 일부 뉴런이 비활성화 되었을 때에도 타깃을 잘 예측하려면, 특정 뉴런에 과도하게 의존하지 않고 모든 뉴런이 의미있는 패턴을 학습해야 함
  - -> 뉴런이 훈련 세트에 있는 패턴을 고르게 감지하므로 **전체적인 일반화 성능이 높아짐**
- 드롭아웃은 **모델을 훈련시킬 때만 적용하는 기법**이므로, 테스트나 실전에서는 적용하지 않음
  - 이로 인해 상대적으로 테스트와 실전의 출력값이 훈련할 때의 출력값보다 높아지므로, 테스트나 실전에서는 출력값을 드롭아웃 비율만큼 낮춰야 함
- **텐서플로에서는 드롭아웃의 비율만큼 뉴런의 출력을 높임**
  - 대부분의 딥러닝 프레임워크는 반대로 문제를 해결
  - **훈련할 때 드롭아웃 비율만큼 뉴런의 출력을 높여 훈련시킴**
  - 테스트에서는 드롭아웃되는 뉴런이 없으므로 출력을 높이지 않음

<br>

#### 드롭아웃 적용해 합성곱 신경망 구현
- 텐서플로에서 드롭아웃을 적용하려면 간단히 `Dropout` 클래스 추가
- **`Dropout` 클래스의 매개변수에 드롭아웃될 비율을 실수로 지정**
- 드롭아웃층에는 학습되는 가중치가 없음
  - 단순히 일부 뉴런의 출력을 무작위로 0으로 만들고, 나머지 뉴런의 출력을 드롭되지 않은 비율로 나누어 증가시킴 
1. **케라스로 만든 합성곱 신경망에 드롭아웃 적용하기**
   - 앞에서 만든 모델에서 합성곱층과 완전 연결층 사이에 드롭아웃층을 추가하여 과대적합의 변화 알아보기
   ```
   from tensorflow.keras.layers import Dropout
   
   conv2 = tf.keras.Sequential()
   conv2.add(Conv2D(10, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))
   conv2.add(MaxPooling2D((2, 2)))
   conv2.add(Flatten())
   conv2.add(Dropout(0.5))
   conv2.add(Dense(100, activation='relu'))
   conv2.add(Dense(10, activation='softmax'))
   ```
2. **드롭아웃층 확인하기**
   - 드롭아웃층은 훈련되는 가중치가 없고, 텐서의 차원을 바꾸지 않음
   ```
   conv2.summary()
   ```
3. **훈련하기**
   - 앞에서 만든 합성곱 신경망 모델과 같은 옵티마이저 사용, 20번의 에포크 동안 훈련
   ```
   conv2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
   history = conv2.fit(x_train, y_train_encoded, epochs=20, validation_data=(x_val, y_val_encoded))
   ```
4. **손실 그래프와 정확도 그래프 그리기**
   ```
   plt.plot(history.history['loss'])
   plt.plot(history.history['val_loss'])
   plt.ylabel('loss')
   plt.xlabel('epoch')
   plt.legend(['train_loss', 'val_loss'])
   plt.show()
   
   plt.plot(history.history['accuracy'])
   plt.plot(history.history['val_accuracy'])
   plt.ylabel('accuracy')
   plt.xlabel('epoch')
   plt.legend(['train_accuracy', 'val_accuracy'])
   plt.show()
   ```
   - 검증 손실이 증가되는 에포크가 확실히 더 늦춰졌고, 훈련 손실과의 차이도 좁혀졌음
   - 정확도는 미세하게 증가
   - 앞에서도 말했듯이 분류 문제에서 정확도를 직접 최적화할 수 없음
   - 크로스 엔트로피 손실 함수를 대신 최적화함
     - 손실 함수를 최소화하면 정확도가 높아질 것으로 기대할 수 있지만 반드시 그렇지는 않음
