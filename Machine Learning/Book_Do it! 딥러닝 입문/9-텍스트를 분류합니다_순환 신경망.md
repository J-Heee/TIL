# Chapter 9. 텍스트를 분류합니다 - 순환 신경망

## 2021.03.02

### 9-1. 순차 데이터와 순환 신경망
- 지금까지 인공신경망에 사용한 데이터는 각 샘플이 독립적이라고 가정함
- 이런 가정 때문에 에포크마다 전체 샘플을 섞은 후에 모델 훈련을 진행할 수 있었음

<br>

#### 순차 데이터(sequential data)
- 우리가 다루는 데이터 중에는 독립적이지 않고, 샘플이 서로 연관되어 있는 경우가 많음
- **시계열(time series) 데이터 : 일정 시간 간격으로 배치된 데이터**
  - ex) 온도를 매시간 측정하여 만든 데이터 세트(각 시간의 온도는 이전 시간의 온도와 깊은 연관이 있을 것)
- **순차 데이터 : 시계열 데이터를 포함하여 샘플에 순서가 있는 데이터**
  - ex) 텍스트(글을 구성하는 글자와 단어들의 순서가 맞아야 의미가 제대로 전달)
- **타임 스텝(time step) : 모델에서 순차 데이터를 처리하는 각 단계**
  - ex) 3개의 단어로 이루어진 순차 데이터의 예 - Hello Deep Learning
    - 처리 단위가 단어일 경우, 총 타임 스텝은 3
    - 처리 단위가 글자일 경우, 총 타입 스텝은 19
- 하지만 완전 연결 신경망이나 합성곱 신경망은 이전의 샘플에 대한 정보를 유지하지 않음
- 순서가 있는 데이터를 처리하기 위해 개발된 것이 순환 신경망

<br>

#### 순환 신경망(recurrent neural network)
- **뉴런의 출력이 순환되는 신경망을 의미**
- **순환 구조 : 은닉층의 출력이 다시 은닉층의 입력으로 사용되는 것**
  - 지금까지 공부한 인공신경망과의 차이점 (책 p.281 그림 참고)
- **순환층: 순환 구조가 있는 층**
- 순환 구조가 순환 신경망에 미치는 영향
  - 은닉층에서 순환된 출력은 다음 입력을 처리할 때 현재 입력과 같이 사용됨
  - **이전 샘플의 정보를 현재 샘플을 처리할 때 참조 가능**한 것
  - 따라서 앞의 샘플을 고려하여 현재 샘플을 처리 가능
- **순환 신경망에서는 층이나 뉴런을 셀(cell)이라고 부름**
  - 각 뉴런마다 순환 구조를 표현하기 번거로우므로, 셀 하나에 순환 구조를 나타내는 경우가 많음 (실제로는 여러 개의 뉴런을 사용하는 것)
  - **은닉 상태(hidden state) : 순환 신경망에서의 셀의 출력**
  - 책 p.282 순환층의 그림 참고
    - 입력 x, 현재 은닉 상태 h, 이전 타임 스텝의 은닉 상태 h<sub>p</sub>
    - 현재의 은닉 상태 h를 계산하기 위해 이전 타임 스텝의 은닉 상태를 사용
    - 현재의 은닉 상태 h는 다음 입력을 처리할 때 다시 사용
  - 순환층의 셀에서 수행되는 계산은 지금까지 공부한 정방향 계산과 배우 비슷하지만, **이전 타임 스텝의 은닉 상태와 곱하는 가중치가 하나 더 있다는 차이점**
    - **Z = XW<sub>x</sub> + H<sub>p</sub>W<sub>h</sub> + b**
    - 책 p.283 순환층의 셀에서 수행되는 계산 그림 참고
  - 순환 신경망의 셀에서는 **활성화 함수로 하이퍼볼릭 탄젠트(hyperbolic tangent) 함수**를 많이 사용

<br>

#### 순환 신경망의 정방향 계산
- 하나의 순환층과 입력층, 출력층을 가진 단순한 순환 신경망 (책 p.283 그림 참고)
- 각 층의 정방향 계산식
  - 이진 분류 문제를 가정한 신경망이므로, 출력층의 활성화 함수로 시그모이드 함수 사용
  - |순환층의 정방향 계산|출력층의 정방향 계산|
    |:-------------------:|:-------------------:|
    |Z<sub>1</sub> = XW<sub>1x</sub> + H<sub>p</sub>W<sub>1h</sub> + b<sub>1</sub>|Z<sub>2</sub> = HW<sub>2</sub> + b<sub>2</sub>|
    |H = tanh(Z<sub>1</sub>)|A<sub>2</sub> = sigmoid(Z<sub>2</sub>)|
- 정방향 계산식에 필요한 입력과 가중치의 구조
  - 입력 데이터 X의 크기는 (샘플 개수 m, 특성 개수 n<sub>f</sub>)
  - 입력에 곱해지는 가중치 W<sub>1x</sub>의 크기는 (특성 개수 n<sub>f</sub>, 순환층의 셀 개수 n<sub>c</sub>)
  - 입력 데이터 X와 가중치 W<sub>1x</sub>에 점 곱 연산을 적용한 결과의 크기는 (m, n<sub>c</sub>)
  - Z<sub>1</sub>, H, 그리고 H<sub>p</sub>의 크기도 모두 (m, n<sub>c</sub>)
  - 가중치 W<sub>1h</sub>의 크기는 (n<sub>c</sub>, n<sub>c</sub>)
  - H<sub>p</sub>W<sub>1h</sub> = (m, n<sub>c</sub>)
  - W<sub>2</sub>의 크기는 (n<sub>c</sub>, n_classes)
  - Z<sub>2</sub>, A<sub>2</sub>의 크기는 (m, n_classes)
  - HW<sub>2</sub> = (m, 1)
  - b<sub>1</sub>의 크기는 (n<sub>c</sub>, )이고, b<sub>2</sub>의 크기는 (n_classes_, )

<br>

#### 순환 신경망의 역방향 계산
- 가중치 W<sub>2</sub>에 대한 손실 함수의 도함수 구하기
- H에 대한 Z<sub>2</sub>의 도함수 구하기
- Z<sub>1</sub>에 대한 H의 도함수 구하기
- 가중치 W<sub>1h</sub>에 대한 Z<sub>1</sub>의 도함수 구하기
- 가중치 W<sub>1x</sub>에 대한 Z<sub>1</sub>의 도함수 구하기
- 절편 b<sub>1</sub>에 대한 Z<sub>1</sub>의 도함수 구하기

<br>
<br>

### 9-2. 순환 신경망을 만들고 텍스트 분류
- 텍스트 데이터를 순환 신경망에 사용하기 위해서는 적절한 방법으로 변환 필요

<br>

#### 훈련 세트와 검증 세트 준비
- **IMDB 데이터 세트**는 인터넷 영화 데이터베이스(Internet Movie Database)에서 수집한 **영화 리뷰 데이터**
- 순환 신경망으로 이 리뷰들이 **긍정적인지 부정적인지를 판별하는 이진 분류 문제**를 해결할 것
- IMDB 데이터 세트는 훈련 세트 25,000개, 테스트 세트 25,000개로 구성되어 있으며, 훈련 세트에서 5,000개의 세트를 분리하여 검증 세트로 사용
1. **텐서플로에서 IMDB 데이터 세트 불러오기**
   - IMDB 데이터 세트는 텐서플로에서 바로 불러올 수 있음
   - 다음 코드를 실행하면 IMDB 데이터 세트를 인터넷에서 내려받을 수 있음
   - IMDB 데이터 세트는 리뷰에 포함된 **80,000개 이상의 단어를 고유한 정수로** 미리 바꾸어 놓았음
   ```
   import numpy as np
   from tensorflow.keras.datasets import imdb
   
   (x_train_all, y_train_all), (x_test, y_test) = imdb.load_data(skip_top=20, num_words=100)
   ```
   - `load_data()` 함수의 매개변수
     - `skip_top` 매개변수 : 가장 많이 등장한 단어들 중 **건너뛸 단어의 개수** 지정
     - `num_words` 매개변수 : **훈련에 사용할 단어의 개수**를 지정
       - 이 실습에서는 메모리를 절약하기 위해 `num_words`에 100 지정 
2. **데이터 세트를 불러왔으므로 훈련 세트의 크기 확인하기**
   ```
   print(x_train_all.shape, y_train_all.shape)

   # 출력
   # (25000,) (25000,)
   ```
3. **훈련 세트의 샘플 확인하기**
   - 영단어가 아니라 정수가 나타남
     - 이 정수들은 **영단어를 고유한 정수에 일대일 대응한 것**으로, **BoW(Bag of Word)** 혹은 **어휘 사전**이라고 부름
   - 훈련 세트에서 눈에 띄는 숫자인 **2는 어휘 사전에 없는 단어**를 의미
   - 가장 많이 등장하는 영단어 20개를 건너뛰고 100개의 단어만 선택했기 때문에 사전에 없는 영단어가 많음
   ```
   print(x_train_all[0])

   # 출력
   [2, 2, 22, 2, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 2, 2, 36, 2, 2, 25, 2, 43, 2, 2, 50, 2, 2, 2, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 39, 2, 2, 2, 2, 2, 2, 38, 2, 2, 2, 2, 50, 2, 2, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 22, 71, 87, 2, 2, 43, 2, 38, 76, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 2, 2, 2, 2, 62, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 66, 2, 33, 2, 2, 2, 2, 38, 2, 2, 25, 2, 51, 36, 2, 48, 25, 2, 33, 2, 22, 2, 2, 28, 77, 52, 2, 2, 2, 2, 82, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 2, 2, 2, 2, 2, 2, 88, 2, 2, 2, 2, 98, 32, 2, 56, 26, 2, 2, 2, 2, 2, 2, 2, 22, 21, 2, 2, 26, 2, 2, 2, 30, 2, 2, 51, 36, 28, 2, 92, 25, 2, 2, 2, 65, 2, 38, 2, 88, 2, 2, 2, 2, 2, 2, 2, 2, 32, 2, 2, 2, 2, 2, 32]
   ```
4. **훈련 세트에서 2 제외하기**
   - 숫자 2는 어휘 사전에 없는 단어
   - 추가로 **0은 패딩, 1은 글의 시작**을 나타내는 데 사용
   - 이 숫자들을 제외하고 훈련 세트를 만듦
   ```
   for i in range(len(x_train_all)):
       x_train_all[i] = [w for w in x_train_all[i] if w > 2]
       
   print(x_train_all[0])

   # 출력
   [22, 43, 65, 66, 36, 25, 43, 50, 35, 39, 38, 50, 22, 22, 71, 87, 43, 38, 76, 22, 62, 66, 33, 38, 25, 51, 36, 48, 25, 33, 22, 28, 77, 52, 82, 36, 71, 43, 26, 46, 88, 98, 32, 56, 26, 22, 21, 26, 30, 51, 36, 28, 92, 25, 65, 38, 88, 32, 32]
   ```
5. **어휘 사전 내려받기**
   - 훈련 세트를 쉽게 이해할 수 있도록 영단어로 바꾸기
     - 정수를 영단어로 바꾸려면 어휘 사전이 필요
   - **어휘 사전은 `get_word_index()` 함수로 내려받을 수 있음**
     - 이 함수는 **영단어와 정수로 구성된 딕셔너리를 반환**
   - 어휘 사전을 내려받은 다음 딕셔너리의 키 값을 movie로 지정하여 값을 출력해보면, movie라는 영단어는 17이라는 정수에 대응되어 있음을 알 수 있음
   ```
   word_to_index = imdb.get_word_index()
   word_to_index['movie']

   # 출력
   # 17
   ```
6. **훈련 세트의 정수를 영단어로 변환하기**
   - 훈련 세트에 있는 정수는 3 이상부터 영단어를 의미하므로 **3을 뺀 값을 어휘 사전의 인덱스로 사용**해야 함
   - 훈련 세트를 영단어로 변환하여 출력
   ```
   index_to_word = {word_to_index[k]: k for k in word_to_index}
   
   for w in x_train_all[0]:
       print(index_to_word[w - 3], end=' ')

   # 출력
   # film just story really they you just there an from so there film film were great just so much film would really at so you what they if you at film have been good also they were just are out because them all up are film but are be what they have don't you story so because all all 
   ```
7. **훈련 샘플의 길이 확인하기**
   - **훈련 세트의 입력 데이터는** 넘파이 배열이 아니라 **파이썬 리스트**
     - 각 리뷰들의 길이가 달라 샘플의 길이가 다르기 때문
   - 두 샘플의 길이 직접 확인
   ```
   print(len(x_train_all[0]), len(x_train_all[1]))

   # 출력
   # 59 32
   ```
   - 첫 번째 샘플과 두 번째 샘플의 길이는 각각 59와 32로, 큰 차이가 있음
   - **샘플의 길이가 다르면 모델을 제대로 훈련시킬 수 없음**
8. **훈련 세트의 타깃 데이터 확인하기**
   - 이진 분류 문제이므로 타깃 데이터는 1과 0으로 영화 리뷰가 긍정(1)인지 부정(0)인지를 나타냄
   ```
   print(y_train_all[:10])

   # 출력
   # [1 0 0 1 0 0 1 0 1 0]
   ```
9. **검증 세트 준비하기**
   - 25,000개의 훈련 세트 중 5,000개만 분리하여 검증 세트로 사용할 것
   - **넘파이 `permutation()` 함수**를 사용하여 25,000개의 인덱스를 섞은 후 앞의 20,000개는 훈련 세트로, 나머지는 검증 세트로 분리
   ```
   np.random.seed(42)
   random_index = np.random.permutation(25000)
   
   x_train = x_train_all[random_index[:20000]]
   y_train = y_train_all[random_index[:20000]]
   x_val = x_train_all[random_index[20000:]]
   y_val = y_train_all[random_index[20000:]]
   ```
    
<br>

#### 샘플의 길이 맞추기
- **일정 길이가 넘으면 샘플을 잘라버리고, 길이가 모자라면 0으로 채움**
  - (책 p.294 그림 참고) 샘플의 길이를 7로 동일하게 만들기 위해 두 문장을 0으로 채우거나 자른 예 
  - 길이가 7 이하인 **샘플의 왼쪽에 0을 추가**했다는 것에 주목하기
  - 만약 오른쪽에 0을 추가하면, 이후 샘플이 순환 신경망에 주입될 때 0이 마지막에 주입되므로 모델의 성능이 좋지 않을 것
1. **텐서플로로 샘플의 길이 맞추기**
   - 텐서플로에는 샘플의 길이를 맞추는 작업을 하기 위한 도구가 이미 준비되어 있음
   - **최대 길이를 100으로 설정하여 길이가 동일한 2개의 넘파이 배열** `x_train_seq`, `x_val_seq`를 만듦
   ```
   from tensorflow.keras.preprocessing import sequence
   
   maxlen=100
   x_train_seq = sequence.pad_sequences(x_train, maxlen=maxlen)
   x_val_seq = sequence.pad_sequences(x_val, maxlen=maxlen)
   ```
2. **길이를 조정한 훈련 세트의 크기와 샘플 확인하기**
   - 훈련 세트의 크기를 확인하면 과정1에서 지정한 값으로 샘플의 길이가 변경되었음
   ```
   print(x_train_seq.shape, x_val_seq.shape)

   # 출력
   # (20000, 100) (5000, 100)
   ```
   - 샘플의 길이를 변경한 훈련 세트의 첫 번째 샘플을 확인하면 **왼쪽에 0이 채워져 있음**
   ```
   print(x_train_seq[0])

   # 출력
   [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0  0  0  0  0 35 40 27 28 40 22 83 31 85 45 24 23 31 70 31 76 30 98 32 22 28 51 75 56 30 33 97 53 38 46 53 74 31 35 23 34 22 58]
   ```

<br>

#### 샘플을 원-핫 인코딩하기
- 훈련 데이터를 준비하는 마지막 작업은 정수 데이터를 원-핫 인코딩하는 것
1. **텐서플로로 원-핫 인코딩하기**
   - 텐서플로의 **`to_categorical()` 함수**를 사용하면 원-핫 인코딩 쉽게 가능
   ```
   from tensorflow.keras.utils import to_categorical
   
   x_train_onehot = to_categorical(x_train_seq)
   x_val_onehot = to_categorical(x_val_seq)
   ```
2. **원-핫 인코딩으로 변환된 변수 `x_train_onehot`의 크기 확인**
   - 20,000개의 샘플이 100 차원으로 원-핫 인코딩됨
   ```
   print(x_train_onehot.shape)

   # 출력
   # (20000, 100, 100)
   ```
3. 샘플을 100개의 단어로 제한했지만, `x_train_onehot`의 크기를 확인해 보면 760MB에 다다름
   - 훈련에 사용할 단어의 개수가 늘어나면 컴퓨터의 메모리가 더 필요함
   ```
   print(x_train_onehot.nbytes)

   # 출력
   # 800000000
   ```

<br>

#### 순환 신경망 클래스 구현
- 06장에서 구현했던 `MiniBatchNetwork` 클래스를 기반으로 순환 신경망을 파이썬으로 직접 구현해 볼 것
1. **`__init__()` 메서드 수정하기**
   - 은닉층의 개수 대신 **셀 개수를 입력 받음**
   - 셀에 필요한 가중치 w1h, w1x를 선언
   - 타임 스텝을 거슬러 그레이디언트를 전파하려면, **순환층의 활성화 출력을 모두 가지고 있어야 하므로 변수 h를 선언**
   ```
   def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):
        self.n_cells = n_cells     # 셀 개수
        self.batch_size = batch_size     # 배치 크기
        self.w1h = None            # 은닉 상태에 대한 가중치
        self.w1x = None            # 입력에 대한 가중치
        self.b1 = None             # 순환층의 절편
        self.w2 = None             # 출력층의 가중치
        self.b2 = None             # 출력층의 절편
        self.h = None              # 순환층의 활성화 출력
        self.losses = []           # 훈련 손실
        self.val_losses = []       # 검증 손실
        self.lr = learning_rate    # 학습률
   ```
2. **직교 행렬 방식으로 가중치 초기화하기**
   - 08장에서 글로럿 초기화 방식으로 가중치를 초기화하며 가중치 초기화의 중요성을 강조함
   - **순환 신경망에서는 직교 행렬 초기화(orthogonal initialization)를 사용**
     - 순환 셀에서 은닉 상태를 위한 **가중치가 반복해서 곱해질 때, 너무 커지거나 작아지지 않도록** 만들어줌
   - 텐서플로가 제공하는 가중치 초기화 클래스는 `tensorflow.initializer` 모듈에 들어 있음
     - **직교 행렬 초기화는 `Orthogonal` 클래스**로 제공
     - `Orthogonal` 클래스의 객체를 만든 다음, 생성하려는 가중치 행렬의 크기를 입력하면 가중치의 값이 초기화된 텐서가 반환됨
     - `numpy()` 메서드를 사용해 텐서를 넘파이 배열로 변환하여 가중치 변수에 저장
   ```
   def init_weights(self, n_features, n_classes):
        orth_init = tf.initializers.Orthogonal()
        glorot_init = tf.initializers.GlorotUniform()
        
        self.w1h = orth_init((self.n_cells, self.n_cells)).numpy() # (셀 개수, 셀 개수)
        self.w1x = glorot_init((n_features, self.n_cells)).numpy() # (특성 개수, 셀 개수)
        self.b1 = np.zeros(self.n_cells)                           # 은닉층의 크기
        self.w2 = glorot_init((self.n_cells, n_classes)).numpy()   # (셀 개수, 클래스 개수)
        self.b2 = np.zeros(n_classes)
   ```
3. **정방향 계산 구현하기**
   - 순환 신경망의 정방향 계산을 구현한 코드는 짧지만 분리해서 설명
   ```
   def forpass(self, x):
        self.h = [np.zeros((x.shape[0], self.n_cells))]   # 은닉 상태 초기화
        ...
   ```
   - **각 타임 스텝의 은닉 상태를 저장**하기 위한 **변수 h**를 초기화
     - 이때 은닉 상태의 크기는 **(샘플 개수, 셀 개수)**
   - 역전파 과정을 진행할 때 이전 타임 스텝의 은닉 상태를 사용
     - 첫 번째 타입 스텝의 이전 은닉 상태는 없으므로, 변수 h의 첫 번째 요소에 0으로 채워진 배열을 추가
4. **그런 다음 넘파이의 `swapaxes()` 함수를 사용하여 입력 x의 첫 번째 배치 차원과 두 번째 타임 스텝 차원을 바꿈**
   ```
   ...
   # 배치 차원과 타임 스텝 차원을 바꿈
   seq = np.swapaxes(x, 0, 1)
   ...
   ```
   - 배치 차원과 타임 스텝 차원을 바꾸는 이유는 (책 p.298 그림 참고)
     - 입력 x는 여러 개의 샘플이 담긴 미니 배치
     - 정방향 계산을 할 때는, 한 샘플의 모든 타임 스텝을 처리하고, 그 다음에 샘플을 처리하는 방식이 아님
     - 미니 배치 안에 있는 **모든 샘플의 첫 번째 타임 스텝을 한 번에 처리**하고, **두 번째 타임 스텝을 한 번에 처리**해야 함
       - 이를 손쉽게 구현하기 위해 배치 차원과 타임 스텝 차원을 바꾼 것
5. **마지막으로 각 샘플의 모든 타임 스텝에 대한 정방향 계산 수행**
   - 셀에서 계산된 은닉 상태는 변수 h에 순서대로 추가됨
   ```
   ...
   # 순환층의 선형식 계산
   for x in seq:
        z1 = np.dot(x, self.w1x) + np.dot(self.h[-1], self.w1h) + self.b1
        h = np.tanh(z1)                    # 활성화 함수 적용
        self.h.append(h)                   # 역전파를 위해 은닉 상태 저장
        z2 = np.dot(h, self.w2) + self.b2  # 출력층의 선형 식 계산
   return z2
   ```
6. **역방향 계산 구현하기**
   - 여기서도 **모든 샘플의 타임 스텝을 한 번에 처리하기 위해, 배치 차원과 타임 스텝 차원을 바꿈**
   - `err_to_cell` 변수에 저장되는 값은 Z<sub>1</sub>에 대하여 손실 함수를 미분한 도함수의 결과값
   ```
   def backprop(self, x, err):
        m = len(x)       # 샘플 개수
        
        # 출력층의 가중치와 절편에 대한 그래디언트 계산
        w2_grad = np.dot(self.h[-1].T, err) / m
        b2_grad = np.sum(err) / m
        # 배치 차원과 타임 스텝 차원을 바꿈
        seq = np.swapaxes(x, 0, 1)
        
        w1h_grad = w1x_grad = b1_grad = 0
        # 셀 직전까지 그래디언트 계산
        err_to_cell = np.dot(err, self.w2.T) * (1 - self.h[-1] ** 2)
        # 모든 타임 스텝을 거슬러가면서 그래디언트 전파
        for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):
            w1h_grad += np.dot(h.T, err_to_cell)
            w1x_grad += np.dot(x.T, err_to_cell)
            b1_grad += np.sum(err_to_cell, axis=0)
            # 이전 타임 스텝의 셀 직전까지 그래디언트 계산
            err_to_cell = np.dot(err_to_cell, self.w1h) * (1 - h ** 2)
        
        w1h_grad /= m
        w1x_grad /= m
        b1_grad /= m
    
        return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad
   ```
   - for문에는 슬라이싱 연산 사용됨
     - 그레이디언트는 마지막 타임 스텝부터 거꾸로 적용해야 하므로 `seq[::-1]` 사용
     - 은닉 상태를 저장한 h 변수의 마지막 항목은 for문 이전에 `err_to_cell` 변수를 계산하기 위해 사용했기 때문에 이를 제외하고 `self.h[:-1][::-1]`와 같이 거꾸로 뒤집음
   - `seq` 넘파이 배열과 `self.h` 리스트를 거꾸로 뒤집은 다음, 모든 타임 스텝을 거슬러 올라가지 않음
     - **딱 10개의 타임 스텝만 거슬러 진행함, 왜?**
       - 순환 신경망은 타임 스텝을 거슬러 올라가며 그레이디언트를 전파할 때, **동일한 가중치를 반복적으로 곱함**
       - 이로 인해 그레이디언트가 너무 커지거나 작아지는 문제가 발생하기 쉬움
       - 이를 방지하기 위해 **그레이디언트를 전파하는 타임 스텝의 수를 제한**해야 하는데, 이를 **TBPTT(Truncated Backpropagation Through Time)라고 부름**
7. **나머지 메서드 수정하기**
   - 나머지 다른 메서드들은 06-07장에서 구현한 것과 거의 차이가 없음
- **`RecurrentNetwork` 클래스의 전체 코드**
   ```
   class RecurrentNetwork:
    
    def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):
        self.n_cells = n_cells     # 셀 개수
        self.batch_size = batch_size     # 배치 크기
        self.w1h = None            # 은닉 상태에 대한 가중치
        self.w1x = None            # 입력에 대한 가중치
        self.b1 = None             # 순환층의 절편
        self.w2 = None             # 출력층의 가중치
        self.b2 = None             # 출력층의 절편
        self.h = None              # 순환층의 활성화 출력
        self.losses = []           # 훈련 손실
        self.val_losses = []       # 검증 손실
        self.lr = learning_rate    # 학습률

    def forpass(self, x):
        self.h = [np.zeros((x.shape[0], self.n_cells))]   # 은닉 상태 초기화
        # 배치 차원과 타임 스텝 차원을 바꿈
        seq = np.swapaxes(x, 0, 1)
        # 순환층의 선형식 계산
        for x in seq:
            z1 = np.dot(x, self.w1x) + np.dot(self.h[-1], self.w1h) + self.b1
            h = np.tanh(z1)                    # 활성화 함수 적용
            self.h.append(h)                   # 역전파를 위해 은닉 상태 저장
            z2 = np.dot(h, self.w2) + self.b2  # 출력층의 선형식 계산
        return z2

    def backprop(self, x, err):
        m = len(x)       # 샘플 개수
        
        # 출력층의 가중치와 절편에 대한 그래디언트 계산
        w2_grad = np.dot(self.h[-1].T, err) / m
        b2_grad = np.sum(err) / m
        # 배치 차원과 타임 스텝 차원을 바꿈
        seq = np.swapaxes(x, 0, 1)
        
        w1h_grad = w1x_grad = b1_grad = 0
        # 셀 직전까지 그래디언트 계산
        err_to_cell = np.dot(err, self.w2.T) * (1 - self.h[-1] ** 2)
        # 모든 타임 스텝을 거슬러가면서 그래디언트 전파
        for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):
            w1h_grad += np.dot(h.T, err_to_cell)
            w1x_grad += np.dot(x.T, err_to_cell)
            b1_grad += np.sum(err_to_cell, axis=0)
            # 이전 타임 스텝의 셀 직전까지 그래디언트 계산
            err_to_cell = np.dot(err_to_cell, self.w1h) * (1 - h ** 2)
        
        w1h_grad /= m
        w1x_grad /= m
        b1_grad /= m
    
        return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad
    
    def sigmoid(self, z):
        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해
        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산
        return a
    
    def init_weights(self, n_features, n_classes):
        orth_init = tf.initializers.Orthogonal()
        glorot_init = tf.initializers.GlorotUniform()
        
        self.w1h = orth_init((self.n_cells, self.n_cells)).numpy() # (셀 개수, 셀 개수)
        self.w1x = glorot_init((n_features, self.n_cells)).numpy() # (특성 개수, 셀 개수)
        self.b1 = np.zeros(self.n_cells)                           # 은닉층의 크기
        self.w2 = glorot_init((self.n_cells, n_classes)).numpy()   # (셀 개수, 클래스 개수)
        self.b2 = np.zeros(n_classes)
        
    def fit(self, x, y, epochs=100, x_val=None, y_val=None):
        y = y.reshape(-1, 1)
        y_val = y_val.reshape(-1, 1)
        np.random.seed(42)
        self.init_weights(x.shape[2], y.shape[1])    # 은닉층과 출력층의 가중치 초기화
        # epochs만큼 반복
        for i in range(epochs):
            print('에포크', i, end=' ')
            # 제너레이터 함수에서 반환한 미니배치를 순환
            batch_losses = []
            for x_batch, y_batch in self.gen_batch(x, y):
                print('.', end='')
                a = self.training(x_batch, y_batch)
                # 안전한 로그 계산을 위해 클리핑
                a = np.clip(a, 1e-10, 1-1e-10)
                # 로그 손실과 규제 손실을 더하여 리스트에 추가
                loss = np.mean(-(y_batch*np.log(a) + (1-y_batch)*np.log(1-a)))
                batch_losses.append(loss)
            print()
            self.losses.append(np.mean(batch_losses))
            # 검증 세트에 대한 손실 계산
            self.update_val_loss(x_val, y_val)

    # 미니배치 제너레이터 함수
    def gen_batch(self, x, y):
        length = len(x)
        bins = length // self.batch_size # 미니배치 횟수
        if length % self.batch_size:
            bins += 1                    # 나누어 떨어지지 않을 때
        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞기
        x = x[indexes]
        y = y[indexes]
        for i in range(bins):
            start = self.batch_size * i
            end = self.batch_size * (i + 1)
            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환
            
    def training(self, x, y):
        m = len(x)                # 샘플 개수 저장
        z = self.forpass(x)       # 정방향 계산 수행
        a = self.sigmoid(z)       # 활성화 함수 적용
        err = -(y - a)            # 오차 계산
        # 오차를 역전파하여 그래디언트 계산
        w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
        # 셀의 가중치와 절편 업데이트
        self.w1h -= self.lr * w1h_grad
        self.w1x -= self.lr * w1x_grad
        self.b1 -= self.lr * b1_grad
        # 출력층의 가중치와 절편 업데이트
        self.w2 -= self.lr * w2_grad
        self.b2 -= self.lr * b2_grad
        return a
   
    def predict(self, x):
        z = self.forpass(x)          # 정방향 계산 수행
        return z > 0                 # 스텝 함수 적용
    
    def score(self, x, y):
        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환
        return np.mean(self.predict(x) == y.reshape(-1, 1))

    def update_val_loss(self, x_val, y_val):
        z = self.forpass(x_val)            # 정방향 계산 수행
        a = self.sigmoid(z)                # 활성화 함수 적용
        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값 클리핑
        val_loss = np.mean(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))
        self.val_losses.append(val_loss)
   ```

<br>

#### 순환 신경망 모델 훈련시키기
- 준비한 IMDB 데이터 세트에 `RecurrentNetwork` 클래스 적용해보기
1. **순환 신경망 모델 훈련시키기**
   - 셀 개수는 32개, 배치 크기는 32개, 학습률은 0.01, 에포크 횟수는 20을 사용
   - **이런 값을 포함해 TBPTT를 위한 타임 스텝 횟수는 모두 하이퍼파라미터**
     - 데이터 세트에 따라 반복적인 실험을 통해 적절한 값을 찾아야 함
   ```
   rn = RecurrentNetwork(n_cells=32, batch_size=32, learning_rate=0.01)
   
   rn.fit(x_train_onehot, y_train, epochs=20, x_val=x_val_onehot, y_val=y_val)
   ```
2. **훈련, 검증 세트에 대한 손실 그래프 그리기**
   ```
   import matplotlib.pyplot as plt
   
   plt.plot(rn.losses)
   plt.plot(rn.val_losses)
   plt.show()
   ```
   - 그래프가 조금 들쭉날죽하지만 비교적 손실이 잘 감소되고 있음
3. **검증 세트 정확도 평가하기**
   ```
   rn.score(x_val_onehot, y_val)

   # 출력
   # 0.6572
   ```
   - 성공적으로 순환 신경망 훈련을 마침
   - 영화 리뷰가 긍정인지 부정인지를 무작위로 예측하는 확률(50%)보다는 좋은 성능이 나옴
     - 하지만 실전에 투입하기에는 아쉬운 성능
   - 다음 절에서 텐서플로를 사용해 여러가지 고급 기술을 사용한 순환 신경망을 만들어볼 것

<br>
<br>

### 9-3. 텐서플로로 순환 신경망 만들기

#### SimpleRNN 클래스로 순환 신경망 만들기
- **텐서플로에서 가장 기본적인 순환층은 `SimpleRNN` 클래스**
- 파이썬 클래스로 만들었던 것과 동일한 신경망을 SimpleRNN 클래스를 사용해 만들 것
1. **순환 신경망에 필요한 클래스 임포트하기**
   - `Sequential`, `SimpleRNN` 클래스와 마지막 출력층을 위한 `Dense` 클래스 임포트
   ```
   from tensorflow.keras.models import Sequential
   from tensorflow.keras.layers import Dense, SimpleRNN
   ```
2. **모델 만들기**
   - `SimpleRNN` 클래스의 사용법은 `Dense` 클래스와 비슷
   - 사용할 셀의 개수를 지정하고, `Sequential` 모델의 첫 번째 층이므로 입력 차원을 지정
     - 타임 스텝의 길이가 100이고, 원-핫 인코딩 크기가 100이므로, 입력 크기는 (100, 100)
   - 이진 분류이므로 1개의 유닛을 가진 Dense층을 마지막에 추가
   - 모델을 완성하고 전체 구조를 출력해서 확인
   ```
   model = Sequential()
   
   model.add(SimpleRNN(32, input_shape=(100, 100)))
   model.add(Dense(1, activation='sigmoid'))
   
   model.summary()
   ```
3. **모델 컴파일하고 IMDB 데이터 세트에 훈련시키기**
   - 가장 기본인 확률적 경사 하강법 알고리즘인 `sgd`를 지정
   - 이진 분류이므로 손실 함수는 `binary_crossentropy`로 지정
   ```
   model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
   
   history = model.fit(x_train_onehot, y_train, epochs=20, batch_size=32, validation_data=(x_val_onehot, y_val))
   ```
4. **훈련, 검증 세트에 대한 손실 그래프와 정확도 그래프 그리기**
   ```
   plt.plot(history.history['loss'])
   plt.plot(history.history['val_loss'])
   plt.show()
   
   plt.plot(history.history['accuracy'])
   plt.plot(history.history['val_accuracy'])
   plt.show()
   ```
   - 20번의 에포크 동안 손실과 정확도를 기록한 이 그래프에서 **조금씩 과대 적합되는 현상**이 보임
5. **검증 세트 정확도 평가하기**
   ```
   loss, accuracy = model.evaluate(x_val_onehot, y_val, verbose=0)
   print(accuracy)

   # 출력
   # 0.6998
   ```
   - 검증 세트에 대한 정확도를 평가하면 **약 70%의 정확도**를 보여줌
   - 앞에서 순환 신경망을 직접 구현하여 모델을 훈련시켜 얻은 정확도(65%)에 비해 큰 성능 향상이 느껴지지는 않음
   - 조금 더 향상된 성능을 위한 고급 기술을 알아보자

<br>

#### 임베딩층으로 순환 신경망 모델 성능 높이기
- 앞에서 만든 순환 신경망의 가장 **큰 단점** 중 하나는, **텍스트 데이터를 원-핫 인코딩으로 전처리한다는 것**
  - 원-핫 인코딩을 사용하면, **입력 데이터 크기와 사용할 수 있는 영단어의 수가 제한된다**는 문제
  - 또한, '단어 사이에는 관련이 전혀 없다'는 가정이 전제되어야 함
    - **단어 사이의 유사성을 표현하지 못함**
- 이런 문제를 해결하기 위해 고안된 것이 **단어 임베딩(word embedding)**
  - 단어 임베딩은 **단어를 고정된 길이의 실수 벡터로 임베딩함**
  - 모델을 훈련하면서 같이 훈련되므로, 훈련이 진행될수록 단어의 연관 관계를 더 정확하게 찾을 수 있음
  - 책 p.308 예시 참고
1. **`Embedding` 클래스 임포트하기**
   ```
   from tensorflow.keras.layers import Embedding
   ```
2. **훈련 데이터 준비하기**
   - 앞에서 IMDB 데이터 세트를 전처리했던 과정 되풀이
   - 다만 여기서는 1,000개의 단어 사용
     - **단어 임베딩은** 단어를 표현하는 벡터의 크기를 임의로 지정할 수 있으므로, **사용하는 단어의 개수에 영향을 받지 않음**
   ```
   (x_train_all, y_train_all), (x_test, y_test) = imdb.load_data(skip_top=20, num_words=1000)
   
   for i in range(len(x_train_all)):
       x_train_all[i] = [w for w in x_train_all[i] if w > 2]

   x_train = x_train_all[random_index[:20000]]
   y_train = y_train_all[random_index[:20000]]
   x_val = x_train_all[random_index[20000:]]
   y_val = y_train_all[random_index[20000:]]
   ```
3. **샘플 길이 맞추기**
   - 타임 스텝의 크기가 100인 시퀀스 데이터를 만듦
   ```
   maxlen=100
   x_train_seq = sequence.pad_sequences(x_train, maxlen=maxlen)
   x_val_seq = sequence.pad_sequences(x_val, maxlen=maxlen)
   ```
4. **모델 만들기**
   - 원-핫 인코딩된 입력 벡터의 길이는 100이었지만, 단어 임베딩에서는 길이를 32로 줄임
   - `Embedding` 클래스에 입력한 매개변수는 단어 개수와 출력 길이
   - `SimpleRNN`의 셀 개수를 8개로 줄임
     - 임베딩층의 성능이 뛰어나기 때문에 셀 개수가 적어도 만족할 만한 성능 얻을 수 있음
   ```
   model_ebd = Sequential()
   
   model_ebd.add(Embedding(1000, 32))
   model_ebd.add(SimpleRNN(8))
   model_ebd.add(Dense(1, activation='sigmoid'))
   
   model_ebd.summary()
   ```
5. **모델 컴파일하고 훈련시키기**
   - 훈련 과정은 이전과 동일
   - 여기서는 08장에서 사용했던 `Adam` 최적화 알고리즘을 사용하고, 10번의 에포크만 훈련
   ```
   model_ebd.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
   
   history = model_ebd.fit(x_train_seq, y_train, epochs=10, batch_size=32, validation_data=(x_val_seq, y_val))
   ```
6. **손실 그래프와 정확도 그래프 그리기**
   ```
   plt.plot(history.history['loss'])
   plt.plot(history.history['val_loss'])
   plt.show()
   
   plt.plot(history.history['accuracy'])
   plt.plot(history.history['val_accuracy'])
   plt.show()
   ```
   - **훈련 초기에 이미 성능이 크게 향상**되어, 에포크가 진행됨에 따라 **다소 과대적합**되는 모습을 보임
7. **검증 세트 정확도 평가하기**
   - 원-핫 인코딩을 사용하지 않아 메모리 사용량이 절감되었음
   - 또한, 적은 셀 개수에서도 더 높은 성능을 내었음
   - 단어 임베딩은 효율적이고 성능이 뛰어나기 때문에 **순환 신경망에서 텍스트 처리를 할 때 임베딩층이 기본으로 사용됨**
   ```
   loss, accuracy = model_ebd.evaluate(x_val_seq, y_val, verbose=0)
   print(accuracy)

   # 출력
   # 0.8096
   ```

<br>
<br>

### 9-4. LSTM 순환 신경망을 만들고 텍스트 분류
- 9-1절에서 그레이디언트가 타임 스텝을 거슬러 가며 전파될 때, 가중치가 반복하여 곱해지므로 그레이디언트가 크게 증가하거나 감소하는 현상을 만든다고 했음
- 이를 해결하기 위해 가장 단순한 방법인 **TBPTT**를 구현함
  - 그러나 이 방법은 그레이디언트가 타임 스텝 끝까지 전파되지 않으므로, 타임 스텝이 멀리 떨어진 영단어 사이의 관계를 파악하기 어려움
- 이런 경우, **좀 더 긴 타임 스텝의 데이터를 처리하는 LSTM(Long Short-Term Memory) 순환 신경망을 사용**
  - **그레이디언트 소실 문제를 극복**하여 긴 시퀀스를 성공적으로 모델링할 수 있음

<br>

#### LSTM 셀의 구조
- 기본 순환 신경망과 달리 **LSTM 셀은 2개의 출력이 순환됨**
  - 그중 하나만 다음 층으로 전달
  - 셀로 순환만 되는 출력을 **셀 상태(C)** 라고 부름
- LSTM 셀에서 수행하는 계산은 책 p.313-314 참고
  - LSTM 셀은 복잡한 구조로 되어 있지만, 텐서플로를 사용하면 아주 간단하게 구현 가능

<br>

#### 텐서플로로 LSTM 순환 신경망 만들기
- 9-3절에서는 텐서플로로 순환 신경망을 만들 때 `SimpleRNN` 클래스를 사용
- 이 클래스를 `LSTM` 클래스로 바꾸기만 하면 LSTM 순환 신경망을 만들 수 있음
1. **LSTM 순환 신경망 만들기**
   - `LSTM` 클래스를 임포트하고, 임베딩층을 포함하여 LSTM 순환 신경망을 만듦
   ```
   from tensorflow.keras.layers import LSTM
   
   model_lstm = Sequential()
   
   model_lstm.add(Embedding(1000, 32))
   model_lstm.add(LSTM(8))
   model_lstm.add(Dense(1, activation='sigmoid'))
   
   model_lstm.summary()
   ```
2. **모델 훈련하기**
   - 10번의 에포크 동안 `Adam` 최적화 알고리즘을 사용하여 모델 훈련
   ```
   model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
   
   history = model_lstm.fit(x_train_seq, y_train, epochs=10, batch_size=32, validation_data=(x_val_seq, y_val))
   ```
3. **손실 그래프와 정확도 그래프 그리기**
   ```
   plt.plot(history.history['loss'])
   plt.plot(history.history['val_loss'])
   plt.show()
   
   plt.plot(history.history['accuracy'])
   plt.plot(history.history['val_accuracy'])
   plt.show()
   ```
4. **검증 세트 정확도 평가하기**
   ```
   loss, accuracy = model_lstm.evaluate(x_val_seq, y_val, verbose=0)
   print(accuracy)

   # 출력
   # 0.8274
   ```
   - 성능이 더 향상됨!

<br>
<br>

### Reference
- [Do it! 딥러닝 입문](http://www.yes24.com/Product/Goods/78896574?OzSrank=2) - 박해선 저. 이지스퍼블리싱. 2019